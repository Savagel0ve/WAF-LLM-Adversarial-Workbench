# 强化学习细节

## 1. 初始化（Initialization）

### 模型加载

- 加载第一阶段预训练语言模型（GPT-2），将其参数 $\theta$ 复制两份：
  - **策略网络（Policy Network, $\pi$）**：充当 Actor，负责生成攻击载荷
  - **价值网络（Value Network, $V$）**：充当 Critic，负责估计当前状态的价值，用于计算优势函数
- 注意：策略网络与价值网络通常共享 Transformer 主体参数，仅输出头（Head）不同（图 4c）

### 加载环境

- 加载第二阶段训练好的 **奖励模型（Reward Model）**，作为模拟环境提供反馈信号

---

## 2. 采样与生成（Rollout / Generation）

在每个训练周期（Epoch）中，策略网络与环境交互：

- **状态 ($S_t$)**：当前已生成的 Token 序列
- **动作 ($A_t$)**：策略网络根据当前状态从词表中采样下一个 Token ID
- **批量生成**：连续生成一批攻击载荷（Payloads），记为 $D_k$

---

## 3. 奖励计算（Reward Calculation）

奖励 $R$ 由两部分组成：

### 3.1 WAF 绕过奖励（$R_{WAF}$）

- 当载荷生成完毕（步骤 $T$）后，将其输入奖励模型
- 奖励模型输出该载荷绕过 WAF 的概率值（0~1 的标量），作为最终奖励
- 注：使用奖励模型而不是真实 WAF，是为了提供更密集的反馈信号并加速训练

### 3.2 KL 散度惩罚（KL-Divergence Penalty）

- 为防止模型在优化中“遗忘”攻击语法（即策略网络偏离预训练模型过远），每一步生成都计算当前策略 $\pi$ 与原始预训练模型 $\rho$ 之间的 KL 散度

### 总奖励公式

$$
R_t = R_{WAF} - \beta \cdot KL(\pi_{\theta_k}, \rho)
$$

其中 $\beta$ 为惩罚系数（论文最佳值为 0.2）。

---

## 4. 优势估计（Advantage Estimation）

- 使用价值网络 $V$ 对当前状态估值，并结合总奖励 $R_t$ 计算优势函数 $\hat{A}_t$
- 优势函数用于衡量“采取当前动作比平均情况好多少”

---

## 5. 模型更新（Optimization with PPO）

使用 PPO 算法同时更新策略网络与价值网络：

### 5.1 策略更新（Actor Update）

- 最大化 PPO-Clip 目标函数 $\mathcal{L}^{policy}$
- PPO 通过截断（Clipping）限制每次参数更新幅度，保证训练稳定性

### 5.2 价值更新（Critic Update）

- 最小化价值预测的均方误差 $\mathcal{L}^{value}$，使价值网络更准确估计奖励

### 5.3 梯度下降

- 通过随机梯度下降（SGD）最小化总损失

---

## 6. 迭代与停止（Iteration & Termination）

- 重复“生成 → 奖励 → 更新”的训练流程
- 停止条件：连续多个 Epoch 的平均奖励不再显著增长（进入平台期）

---

通过以上流程，GPTFuzzer 最终获得一个既保留攻击语法（来自预训练与 KL 约束），又能高概率绕过 WAF（来自奖励模型指导）的攻击载荷生成器。