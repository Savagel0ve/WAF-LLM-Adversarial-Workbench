# 奖励模型训练细节

## 1. 核心目标

直接使用 WAF 的测试结果（0 或 1）作为强化学习奖励会导致学习效率低下（稀疏奖励问题）。因此，作者训练一个奖励模型来预测攻击载荷绕过 WAF 的概率（0~1 的标量），作为强化学习的奖励信号。

---

## 2. 数据准备（Data Collection）

### 采样

- 从第一阶段收集或生成的攻击载荷池中随机采样一定数量的载荷

### 打标

- 将载荷发送给目标 WAF（如 ModSecurity）进行实际测试
- Label 1（Bypassing）：WAF 未拦截（如返回 200）
- Label 0（Blocked）：WAF 拦截（如返回 403）

### 数据量

- SQLi：4,000 条
- XSS：2,000 条
- RCE：2,000 条

### 划分

- 70% 训练集 / 15% 验证集 / 15% 测试集

---

## 3. 模型架构（Model Architecture）

### 初始化

- 奖励模型骨干网络（Backbone）使用第一阶段预训练语言模型参数初始化
- 这使奖励模型继承对攻击语法的理解，加速收敛

### 结构修改

- 移除语言建模头（Language Modeling Head）
- 新增奖励预测头（Reward Prediction Head）：线性层连接在序列最后一个 Token 的隐状态 $H_T^n$ 上

### 输出

- 通过 Sigmoid 输出 $[0, 1]$ 标量 $r(\tau)$，表示绕过概率

---

## 4. 训练过程（Training Process）

### 损失函数

使用二元交叉熵损失（Binary Cross-Entropy Loss）：

$$
L_2 = -\frac{1}{|D_r|} \sum_{\tau \in D_r} \left(y \log(r(\tau)) + (1-y) \log(1-r(\tau))\right)
$$

### 超参数设置

- 优化器：Adam
- 学习率（Learning Rate）：线性预热（Linear Warmup）
  - 前 10% 步数从 0 增加到 2e-5
  - 之后线性衰减至 0
- Batch Size：32
- Epochs：4

---

## 5. 性能评估

训练完成后，使用 F1-score 和 AUC（Area Under Curve）评估奖励模型。实验结果显示，奖励模型在 SQLi、XSS、RCE 任务上 AUC 通常 > 98%，说明其能较准确地模拟 WAF 的拦截逻辑。