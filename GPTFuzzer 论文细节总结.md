# GPTFuzzer 论文细节总结

## 数据收集与预处理（Pretraining 前）

在 GPTFuzzer 框架中，预训练之前的数据收集与预处理是整个流程的基础，旨在为 Transformer 模型提供符合特定攻击类型语法特性的高质量语料。以下是根据论文整理的详细步骤。

### 1. 数据收集（Data Collection）

论文提到了两种主要的攻击载荷（Payload）构建方式：

- **手动收集（Manual Collection）**
  - 从已发表的研究论文（如 DeepSQLi 等）或 OWASP 等公开安全资源中收集现有的攻击数据集。
  - 使用成熟的模糊测试工具（如 SqlMap、XSStrike、Dharma、Commix）自动生成攻击载荷作为补充。
- **基于攻击语法生成（Generation From Attack Grammar）**
  - **构建语法**：由专家通过“自下而上”（分析现有载荷提取模式）或“自上而下”（研究攻击原理从零构建）的方式，使用扩展巴科斯范式（EBNF）定义攻击语法。
  - **随机生成**：从语法的起始符号（`start`）开始，随机选择并应用产生式规则，直到所有符号均为终结符，从而生成大量随机载荷。
  - **改进语法**：作者改进了前人（如 ML-Driven）的 SQLi 语法，增加了更多注入上下文、SQL 函数及 Unicode 编码等混淆手段，并为 XSS 和 RCE 编写了新的语法。

### 2. 数据预处理（Preprocessing）

在送入模型训练之前，所有原始载荷需要转化为模型可理解的数值序列：

- **序列类型选择**
  - **短序列（Short Sequence）**：仅包含语法树中的终结符（Terminal symbols），用于模拟手动收集的真实载荷。
  - **长序列（Long Sequence）**：包含语法树中的所有符号（含非终结符），旨在利用语法中蕴含的额外信息（此时方法称为 GPTFuzzer-L）。
- **分词与数值化（Tokenization & Mapping）**
  - **添加特殊标记**：在每个序列的开头和结尾分别添加起始标记 `<start>` 和结束标记 `<end>`。
  - **分词**
    - 对于手动收集的载荷，使用从数据集学习到的分词器或预定义词汇表将其拆分为 Token（例如：将 `0%0band%0bnot` 拆分为 `0`、`%0b`、`and`、`%0b`、`not`）。
    - 对于从语法生成的序列，直接使用语法节点作为 Token，省略分词步骤。
  - **构建词典**：建立一个将每个 Token 映射到唯一整数 ID（Token ID）的词典。
  - **转换**：将载荷转换为对应的整数 ID 序列 \(\tau=\{x_{0},x_{1},...,x_{T}\}\)，作为预训练阶段的直接输入。

---

## 数据量与超参数（预训练 / 奖励模型 / 强化学习）

根据对论文的详细阅读，关于 GPTFuzzer 在预训练、奖励模型及强化学习阶段的数据量和超参数细节整理如下。

### 1. 预训练使用的数据量

预训练阶段旨在让语言模型学习特定攻击类型（如 SQLi、XSS、RCE）的隐式语法规则。

- **核心实验数据量**：在回答 RQ1（有效性）时，SQLi 和 XSS 使用了 **512,000（512K）** 个有效载荷进行预训练。
- **各攻击类型具体规模**
  - **SQLi**：512,000。
  - **XSS**：512,000。
  - **RCE**：37,302（由于该类攻击空间较小，使用全量数据集）。
- **消融实验发现**：研究表明，当预训练数据量达到 **256K 至 512K** 时，模型能获得（几乎）最优的结果。如果完全没有预训练（0K），模型生成有效载荷的效果极差。

### 2. 奖励模型使用的数据量

奖励模型通过模拟 WAF 的响应（预测绕过概率），为强化学习提供更丰富的信号。

- **SQL 注入（SQLi）**：使用 **4,000** 条标记（Labeled）载荷进行训练。
- **跨站脚本（XSS）**：使用 **2,000** 条标记载荷进行训练。
- **远程命令执行（RCE）**：使用 **2,000** 条标记载荷进行训练。
- **数据划分比例**：奖励模型数据集按 **70% 训练集、15% 验证集、15% 测试集** 进行划分。

### 3. 强化学习（PPO）最佳超参数及细节

强化学习阶段采用 **近端策略优化算法（PPO）** 来微调策略网络。

#### 核心超参数设置

| 参数名称 | 最佳取值 | 备注 |
| --- | --- | --- |
| **KL 散度惩罚系数（\(\beta\)）** | **0.2** | 关键参数，用于平衡绕过率（ER）和多样性（NRR），防止模型陷入局部最优。 |
| **学习率（Learning Rate）** | **\(1.4 \times 10^{-5}\)** | 用于更新策略网络参数。 |
| **批次大小（Batch Size）** | **256** | PPO 训练的 batch size。 |
| **剪切阈值（\(\epsilon\)）** | **0.2** | PPO 算法标准设置。 |
| **最大步数（\(T_{max}\)）** | 预定义值 | 限制生成的 Token ID 序列长度。 |

#### 训练细节

- **模型架构**：策略网络和价值网络共享一个 **GPT-2** 架构的 Transformer 解码器本体（12 层，768 隐层维度，12 个注意力头）。
- **奖励构成**：总奖励结合了奖励模型输出的 **WAF 绕过概率**（仅在最后一步 \(T\) 计算）和中间步骤的 **负 KL 散度惩罚**。
- **停止准则**：当连续 3 个 Epoch 的平均奖励增长率低于或等于预设阈值 \(\delta\) 时，训练停止。

