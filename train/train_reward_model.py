"""
å¥–åŠ±æ¨¡å‹è®­ç»ƒ - æ”¯æŒ Qwen2.5-Coder å’Œå…¶ä»–ç°ä»£ LLM

æ ¹æ®GPTFuzzerè®ºæ–‡:
- åŸºäºé¢„è®­ç»ƒæ¨¡å‹ï¼ˆQwen2.5-Coder æˆ–å…¶ä»–ï¼‰
- è®­ç»ƒåºåˆ—åˆ†ç±»å™¨é¢„æµ‹WAFç»•è¿‡æ¦‚ç‡
- ä½¿ç”¨BCEWithLogitsLoss
- è¾“å‡º r(Ï„) âˆˆ [0, 1]
"""
import torch
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
from datasets import Dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    set_seed,
    EarlyStoppingCallback,
)
import logging


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RewardModelConfig:
    """å¥–åŠ±æ¨¡å‹é…ç½®"""
    
    def __init__(self, args):
        self.pretrained_model_path = args.pretrained_model_path
        self.data_path = args.data_path
        self.output_dir = args.output_dir
        
        # è®ºæ–‡è¶…å‚æ•°
        self.max_length = args.max_length
        self.batch_size = args.batch_size
        self.learning_rate = args.learning_rate
        self.epochs = args.epochs
        self.warmup_ratio = args.warmup_ratio
        self.weight_decay = args.weight_decay
        
        # ç²¾åº¦é…ç½®
        self.seed = args.seed
        self.fp16 = getattr(args, 'fp16', False)
        self.bf16 = getattr(args, 'bf16', True)  # é»˜è®¤ä½¿ç”¨ bf16
        self.early_stopping_patience = args.early_stopping_patience
        
        logger.info("å¥–åŠ±æ¨¡å‹é…ç½®:")
        for key, value in self.__dict__.items():
            logger.info(f"  {key}: {value}")


def load_and_process_data(config: RewardModelConfig, tokenizer):
    """
    åŠ è½½CSVæ•°æ®å¹¶è½¬æ¢ä¸ºHF Datasetæ ¼å¼
    
    æ”¯æŒä¸¤ç§è¾“å…¥æ ¼å¼:
    1. å•ä¸ªCSVæ–‡ä»¶ - è‡ªåŠ¨åˆ’åˆ†train/val/test
    2. å¤šä¸ªCSVæ–‡ä»¶ - train.csv, val.csv, test.csv
    """
    data_path = Path(config.data_path)
    
    if data_path.is_file():
        # å•ä¸ªæ–‡ä»¶ - è‡ªåŠ¨åˆ’åˆ†
        logger.info(f"åŠ è½½æ•°æ®æ–‡ä»¶: {data_path}")
        df = pd.read_csv(data_path)
        
        # éšæœºåˆ’åˆ†
        df = df.sample(frac=1, random_state=config.seed).reset_index(drop=True)
        
        train_size = int(0.7 * len(df))
        val_size = int(0.15 * len(df))
        
        train_df = df[:train_size]
        val_df = df[train_size:train_size + val_size]
        test_df = df[train_size + val_size:]
        
    else:
        # å¤šä¸ªæ–‡ä»¶
        logger.info(f"ä»ç›®å½•åŠ è½½æ•°æ®: {data_path}")
        
        # æŸ¥æ‰¾æ–‡ä»¶
        train_file = None
        val_file = None
        test_file = None
        
        for file in data_path.glob("*.csv"):
            if "train" in file.name:
                train_file = file
            elif "val" in file.name:
                val_file = file
            elif "test" in file.name:
                test_file = file
        
        if not train_file:
            raise ValueError(f"æ‰¾ä¸åˆ°è®­ç»ƒæ–‡ä»¶åœ¨ {data_path}")
        
        train_df = pd.read_csv(train_file)
        val_df = pd.read_csv(val_file) if val_file else None
        test_df = pd.read_csv(test_file) if test_file else None
    
    logger.info(f"æ•°æ®ç»Ÿè®¡: Train={len(train_df)}, Val={len(val_df) if val_df is not None else 0}, Test={len(test_df) if test_df is not None else 0}")
    
    # æ£€æŸ¥åˆ—
    if "text" not in train_df.columns or "label" not in train_df.columns:
        raise ValueError("CSVæ–‡ä»¶å¿…é¡»åŒ…å« 'text' å’Œ 'label' åˆ—")
    
    # æ•°æ®ç»Ÿè®¡
    logger.info(f"è®­ç»ƒé›†æ­£æ ·æœ¬æ¯”ä¾‹: {train_df['label'].mean():.2%}")
    if val_df is not None:
        logger.info(f"éªŒè¯é›†æ­£æ ·æœ¬æ¯”ä¾‹: {val_df['label'].mean():.2%}")
    if test_df is not None:
        logger.info(f"æµ‹è¯•é›†æ­£æ ·æœ¬æ¯”ä¾‹: {test_df['label'].mean():.2%}")
    
    # è½¬æ¢ä¸ºDataset
    datasets_dict = {"train": Dataset.from_pandas(train_df)}
    if val_df is not None:
        datasets_dict["validation"] = Dataset.from_pandas(val_df)
    if test_df is not None:
        datasets_dict["test"] = Dataset.from_pandas(test_df)
    
    raw_datasets = DatasetDict(datasets_dict)
    
    # Tokenize
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=config.max_length,
        )
    
    logger.info("Tokenizingæ•°æ®é›†...")
    tokenized_datasets = raw_datasets.map(
        tokenize_function,
        batched=True,
        desc="Tokenizing"
    )
    
    return tokenized_datasets


def compute_metrics(eval_pred):
    """
    è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    
    æŒ‡æ ‡åŒ…æ‹¬:
    - Accuracy
    - Precision
    - Recall
    - F1-Score
    - AUC-ROC
    """
    logits, labels = eval_pred
    
    # è½¬æ¢ä¸ºæ¦‚ç‡
    probs = torch.sigmoid(torch.tensor(logits)).numpy().flatten()
    predictions = (probs > 0.5).astype(int)
    
    # è®¡ç®—æŒ‡æ ‡
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average="binary", zero_division=0
    )
    acc = accuracy_score(labels, predictions)
    
    # AUC
    try:
        auc = roc_auc_score(labels, probs)
    except Exception as e:
        logger.warning(f"æ— æ³•è®¡ç®—AUC: {e}")
        auc = 0.0
    
    return {
        "accuracy": acc,
        "f1": f1,
        "precision": precision,
        "recall": recall,
        "auc": auc,
    }


class RewardTrainer(Trainer):
    """Custom Trainer - using BCEWithLogitsLoss"""
    
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """Compute loss"""
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        
        # BCEWithLogitsLoss - numerically stable version
        loss_fct = torch.nn.BCEWithLogitsLoss()
        loss = loss_fct(logits.view(-1), labels.float().view(-1))
        
        return (loss, outputs) if return_outputs else loss


def train(args):
    """è®­ç»ƒå¥–åŠ±æ¨¡å‹ - æ”¯æŒ Qwen2.5-Coder å’Œå…¶ä»–ç°ä»£ LLM"""
    config = RewardModelConfig(args)
    set_seed(config.seed)
    
    logger.info("="*60)
    logger.info("ğŸ¯ å¼€å§‹è®­ç»ƒå¥–åŠ±æ¨¡å‹")
    logger.info("="*60)
    
    # åŠ è½½ tokenizer
    logger.info(f"ğŸ“ åŠ è½½ tokenizer: {config.pretrained_model_path}")
    tokenizer = AutoTokenizer.from_pretrained(
        config.pretrained_model_path,
        trust_remote_code=True,
        padding_side="right",  # åˆ†ç±»ä»»åŠ¡ä½¿ç”¨ right padding
    )
    
    # è®¾ç½® pad_token
    if tokenizer.pad_token is None:
        if tokenizer.eos_token:
            tokenizer.pad_token = tokenizer.eos_token
            logger.info(f"   è®¾ç½® pad_token ä¸º eos_token: {tokenizer.eos_token}")
        else:
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})
            logger.info("   æ·»åŠ äº†æ–°çš„ pad_token: [PAD]")
    
    # åŠ è½½å¹¶å¤„ç†æ•°æ®
    tokenized_datasets = load_and_process_data(config, tokenizer)
    
    # ç¡®å®šç²¾åº¦
    torch_dtype = torch.bfloat16 if config.bf16 else (torch.float16 if config.fp16 else torch.float32)
    
    # åŠ è½½æ¨¡å‹
    logger.info(f"ğŸ¤– åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {config.pretrained_model_path}")
    model = AutoModelForSequenceClassification.from_pretrained(
        config.pretrained_model_path,
        num_labels=1,  # äºŒåˆ†ç±»ï¼Œä½¿ç”¨BCEWithLogitsLoss
        trust_remote_code=True,
        torch_dtype=torch_dtype,
        device_map="auto",
    )
    
    # ç¡®ä¿ pad_token_id è®¾ç½®æ­£ç¡®
    if model.config.pad_token_id is None:
        model.config.pad_token_id = tokenizer.pad_token_id
    
    # å¦‚æœæ·»åŠ äº†æ–°çš„ç‰¹æ®Štokenï¼Œéœ€è¦è°ƒæ•´embedding
    if len(tokenizer) > model.config.vocab_size:
        model.resize_token_embeddings(len(tokenizer))
    
    # æ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.info(f"   æ€»å‚æ•°: {total_params / 1e9:.2f}B ({total_params / 1e6:.1f}M)")
    logger.info(f"   å¯è®­ç»ƒ: {trainable_params / 1e9:.2f}B ({trainable_params / 1e6:.1f}M)")
    
    # è®¾å¤‡ä¿¡æ¯
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    if device == "cuda":
        logger.info(f"   GPU: {torch.cuda.get_device_name(0)}")
        logger.info(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=config.output_dir,
        num_train_epochs=config.epochs,
        per_device_train_batch_size=config.batch_size,
        per_device_eval_batch_size=config.batch_size,
        learning_rate=config.learning_rate,
        weight_decay=config.weight_decay,
        warmup_ratio=config.warmup_ratio,
        
        # æ—¥å¿—å’Œä¿å­˜
        logging_dir=f"{config.output_dir}/logs",
        logging_steps=10,
        logging_first_step=True,
        
        # è¯„ä¼°
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        
        # ç²¾åº¦é…ç½® - ä¼˜å…ˆä½¿ç”¨ bf16
        bf16=config.bf16 and torch.cuda.is_available(),
        fp16=config.fp16 and not config.bf16 and torch.cuda.is_available(),
        dataloader_num_workers=4,
        
        # æŠ¥å‘Š
        report_to="tensorboard",
        
        # å…¶ä»–
        seed=config.seed,
        save_total_limit=3,
    )
    
    # Callbacks
    callbacks = []
    if config.early_stopping_patience > 0:
        callbacks.append(
            EarlyStoppingCallback(early_stopping_patience=config.early_stopping_patience)
        )
    
    # åˆ›å»ºTrainer
    trainer = RewardTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets.get("validation", None),
        compute_metrics=compute_metrics,
        callbacks=callbacks,
    )
    
    # è®­ç»ƒ
    logger.info("\nå¼€å§‹è®­ç»ƒ...")
    train_result = trainer.train()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = Path(config.output_dir) / "final_reward_model"
    trainer.save_model(final_model_path)
    tokenizer.save_pretrained(final_model_path)
    logger.info(f"âœ… æ¨¡å‹ä¿å­˜è‡³: {final_model_path}")
    
    # è®­ç»ƒæŒ‡æ ‡
    logger.info("\n" + "="*60)
    logger.info("è®­ç»ƒç»“æœ:")
    for key, value in train_result.metrics.items():
        logger.info(f"  {key}: {value}")
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    if "test" in tokenized_datasets:
        logger.info("\n" + "="*60)
        logger.info("åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
        test_results = trainer.predict(tokenized_datasets["test"])
        
        logger.info("æµ‹è¯•é›†ç»“æœ:")
        for key, value in test_results.metrics.items():
            if isinstance(value, float):
                logger.info(f"  {key}: {value:.4f}")
            else:
                logger.info(f"  {key}: {value}")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        test_results_path = Path(config.output_dir) / "test_results.json"
        import json
        with open(test_results_path, 'w') as f:
            json.dump(test_results.metrics, f, indent=2)
    
    logger.info("\nâœ… è®­ç»ƒå®Œæˆ!")
    logger.info(f"æŸ¥çœ‹è®­ç»ƒæ—¥å¿—: tensorboard --logdir {config.output_dir}/logs")


def main():
    parser = argparse.ArgumentParser(
        description="è®­ç»ƒWAFç»•è¿‡å¥–åŠ±æ¨¡å‹ - æ”¯æŒ Qwen2.5-Coder å’Œå…¶ä»–ç°ä»£ LLM",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨ Qwen2.5-Coder é¢„è®­ç»ƒæ¨¡å‹
  python train_reward_model.py \\
      --pretrained_model_path ./models/pretrain_sqli_qwen2_5_coder_1_5b \\
      --data_path ./data/labeled \\
      --output_dir ./models/reward_sqli_qwen

  # ä½¿ç”¨ BF16 ç²¾åº¦ (æ¨è)
  python train_reward_model.py \\
      --pretrained_model_path ./models/pretrain_sqli_qwen2_5_coder_1_5b \\
      --data_path ./data/labeled \\
      --bf16
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument(
        "--pretrained_model_path",
        type=str,
        required=True,
        help="é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ (Qwen2.5-Coder æˆ–å…¶ä»–)"
    )
    parser.add_argument(
        "--data_path",
        type=str,
        required=True,
        help="æ ‡è®°æ•°æ®è·¯å¾„ (CSVæ–‡ä»¶æˆ–ç›®å½•)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./models/reward_model",
        help="è¾“å‡ºç›®å½•"
    )
    
    # è¶…å‚æ•° (è®ºæ–‡é»˜è®¤å€¼)
    parser.add_argument("--max_length", type=int, default=256, help="æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 256)")
    parser.add_argument("--batch_size", type=int, default=16, help="æ‰¹æ¬¡å¤§å° (é»˜è®¤: 16ï¼ŒQwen 1.5B æ¨è)")
    parser.add_argument("--learning_rate", type=float, default=2e-5, help="å­¦ä¹ ç‡")
    parser.add_argument("--epochs", type=int, default=4, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--warmup_ratio", type=float, default=0.1, help="é¢„çƒ­æ¯”ä¾‹")
    parser.add_argument("--weight_decay", type=float, default=0.01, help="æƒé‡è¡°å‡")
    
    # ç²¾åº¦é…ç½®
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--bf16", action="store_true", default=True, help="ä½¿ç”¨ BF16 ç²¾åº¦ (é»˜è®¤å¼€å¯)")
    parser.add_argument("--fp16", action="store_true", help="ä½¿ç”¨ FP16 ç²¾åº¦")
    parser.add_argument("--no-bf16", action="store_true", help="ç¦ç”¨ BF16")
    parser.add_argument("--early_stopping_patience", type=int, default=0, help="æ—©åœpatience (0=ä¸ä½¿ç”¨)")
    
    args = parser.parse_args()
    
    # å¤„ç†ç²¾åº¦å‚æ•°
    if args.no_bf16:
        args.bf16 = False
    if args.fp16:
        args.bf16 = False
    
    train(args)


if __name__ == "__main__":
    main()
