"""
è®­ç»ƒé…ç½®æ–‡ä»¶ - æ”¯æŒå¤šç§ GPU é…ç½®
- æœ¬åœ°: RTX 4070 8GB
- æœåŠ¡å™¨: RTX 4090D 24GB
æ”¯æŒ Qwen2.5-Coder å’Œå…¶ä»–ç°ä»£ LLM
"""
import os
from dataclasses import dataclass, field
from typing import Optional, List


# ==================== æ¨¡å‹é¢„è®¾ ====================
MODEL_PRESETS = {
    # Qwen2.5-Coder ç³»åˆ— (æ¨è)
    "qwen2.5-coder-0.5b": {
        "model_name": "Qwen/Qwen2.5-Coder-0.5B",
        "max_length": 128,  # å‡å°åºåˆ—é•¿åº¦
        "batch_size": 16,   # å®‰å…¨çš„ batch size
        "gradient_accumulation": 2,  # ç­‰æ•ˆ batch=32
        "use_flash_attention": False,
    },
    "qwen2.5-coder-1.5b": {
        "model_name": "Qwen/Qwen2.5-Coder-1.5B",
        "max_length": 256,
        "batch_size": 4,    # é»˜è®¤å° batch (8GB æ˜¾å­˜)
        "gradient_accumulation": 8,
        "use_flash_attention": False,
        "gradient_checkpointing": True,
    },
    # RTX 4090 24GB ä¼˜åŒ–é…ç½®
    "qwen2.5-coder-1.5b-server": {
        "model_name": "Qwen/Qwen2.5-Coder-1.5B",
        "max_length": 512,   # å¢åŠ åºåˆ—é•¿åº¦ (24GB å……è¶³)
        "batch_size": 24,    # RTX 4090 å¤§ batch (24GB æ˜¾å­˜)
        "gradient_accumulation": 4,  # ç­‰æ•ˆ batch=96
        "use_flash_attention": True,  # RTX 4090 å®Œç¾æ”¯æŒ
        "gradient_checkpointing": False,
    },
    "qwen2.5-coder-3b-server": {
        "model_name": "Qwen/Qwen2.5-Coder-3B",
        "max_length": 256,   # å‡å°åºåˆ—é•¿åº¦èŠ‚çœæ˜¾å­˜
        "batch_size": 8,     # RTX 4090 3B æ¨¡å‹é€‚é… 24GB
        "gradient_accumulation": 12, # ç­‰æ•ˆ batch=96
        "use_flash_attention": True,  # RTX 4090 å®Œç¾æ”¯æŒ
        "gradient_checkpointing": True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜
    },
    # RTX 4090 ä¸“å± - 7B æ¨¡å‹ä¹Ÿå¯ä»¥è·‘
    "qwen2.5-coder-7b-server": {
        "model_name": "Qwen/Qwen2.5-Coder-7B",
        "max_length": 256,
        "batch_size": 4,
        "gradient_accumulation": 16,  # ç­‰æ•ˆ batch=64
        "use_flash_attention": True,
        "gradient_checkpointing": True,  # 7B éœ€è¦æ¢¯åº¦æ£€æŸ¥ç‚¹
    },
    "qwen2.5-coder-3b": {
        "model_name": "Qwen/Qwen2.5-Coder-3B",
        "max_length": 256,
        "batch_size": 2,
        "gradient_accumulation": 16,
        "use_flash_attention": False,
        "load_in_4bit": True,  # Requires quantization
    },
    # Qwen2.5 general series
    "qwen2.5-0.5b": {
        "model_name": "Qwen/Qwen2.5-0.5B",
        "max_length": 512,
        "batch_size": 8,
        "gradient_accumulation": 4,
        "use_flash_attention": False,
    },
    "qwen2.5-1.5b": {
        "model_name": "Qwen/Qwen2.5-1.5B",
        "max_length": 256,
        "batch_size": 4,
        "gradient_accumulation": 8,
        "use_flash_attention": False,
    },
    # DeepSeek Coder
    "deepseek-coder-1.3b": {
        "model_name": "deepseek-ai/deepseek-coder-1.3b-base",
        "max_length": 256,
        "batch_size": 4,
        "gradient_accumulation": 8,
        "use_flash_attention": False,
    },
    # Phi-3
    "phi-3-mini": {
        "model_name": "microsoft/Phi-3-mini-4k-instruct",
        "max_length": 256,
        "batch_size": 2,
        "gradient_accumulation": 16,
        "use_flash_attention": False,
        "load_in_4bit": True,
    },
    # æ—§ç‰ˆ GPT-2 (å…¼å®¹)
    "gpt2": {
        "model_name": "gpt2",
        "max_length": 128,
        "batch_size": 4,
        "gradient_accumulation": 8,
        "use_flash_attention": False,
    },
    "gpt2-medium": {
        "model_name": "gpt2-medium",
        "max_length": 128,
        "batch_size": 2,
        "gradient_accumulation": 16,
        "use_flash_attention": False,
    },
}

# é»˜è®¤æ¨¡å‹
DEFAULT_MODEL = "qwen2.5-coder-1.5b"


@dataclass
class GPUConfig:
    """GPUå’Œæ˜¾å­˜é…ç½®"""
    # æ˜¾å­˜é™åˆ¶
    total_memory_gb: float = 8.0
    threshold_gb: float = 7.5  # è­¦å‘Šé˜ˆå€¼
    
    # æ˜¾å­˜ä¼˜åŒ–
    use_fp16: bool = True  # æ··åˆç²¾åº¦è®­ç»ƒ(èŠ‚çœ50%æ˜¾å­˜)
    use_bf16: bool = False  # BF16 (Ampere+ GPUï¼Œæ›´ç¨³å®š)
    use_8bit_optimizer: bool = True  # 8-bitä¼˜åŒ–å™¨(èŠ‚çœ30-40%æ˜¾å­˜)
    gradient_checkpointing: bool = False  # æ¢¯åº¦æ£€æŸ¥ç‚¹(èŠ‚çœæ˜¾å­˜ä½†é™ä½é€Ÿåº¦)
    
    # é‡åŒ–é…ç½®
    load_in_4bit: bool = False  # 4-bit é‡åŒ– (éœ€è¦ bitsandbytes)
    load_in_8bit: bool = False  # 8-bit é‡åŒ–
    bnb_4bit_compute_dtype: str = "float16"  # 4-bit è®¡ç®—ç²¾åº¦
    bnb_4bit_quant_type: str = "nf4"  # é‡åŒ–ç±»å‹
    
    # Flash Attention
    use_flash_attention: bool = True  # ä½¿ç”¨ Flash Attention 2 (éœ€è¦æ”¯æŒ)
    
    # DeepSpeedé…ç½®(å¯é€‰)
    use_deepspeed: bool = False
    deepspeed_config: Optional[str] = None


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    # æ¨¡å‹é€‰æ‹© - é»˜è®¤ä½¿ç”¨ Qwen2.5-Coder-1.5B
    model_name: str = "Qwen/Qwen2.5-Coder-1.5B"
    model_preset: str = DEFAULT_MODEL  # ä½¿ç”¨é¢„è®¾é…ç½®
    
    # åºåˆ—é•¿åº¦
    max_length: int = 256
    
    # Tokenizer
    tokenizer_name: Optional[str] = None  # é»˜è®¤ä¸model_nameç›¸åŒ
    trust_remote_code: bool = True  # Qwen æ¨¡å‹éœ€è¦
    
    # ç‰¹æ®Štokené…ç½®
    pad_token: Optional[str] = None  # è‡ªåŠ¨å¤„ç†
    
    # æ”»å‡»ç±»å‹
    attack_types: List[str] = field(default_factory=lambda: ["sqli", "xss", "rce"])
    
    @classmethod
    def from_preset(cls, preset_name: str):
        """ä»é¢„è®¾åˆ›å»ºé…ç½®"""
        if preset_name not in MODEL_PRESETS:
            raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹é¢„è®¾: {preset_name}, å¯ç”¨: {list(MODEL_PRESETS.keys())}")
        
        preset = MODEL_PRESETS[preset_name]
        return cls(
            model_name=preset["model_name"],
            model_preset=preset_name,
            max_length=preset.get("max_length", 256),
        )


@dataclass
class TrainingConfig:
    """é¢„è®­ç»ƒé…ç½® - é’ˆå¯¹8GBæ˜¾å­˜ä¼˜åŒ–ï¼Œæ”¯æŒ Qwen2.5-Coder"""
    # è¾“å‡ºç›®å½•
    output_dir: str = "models/pretrain_sqli_qwen"
    
    # è®­ç»ƒè¶…å‚æ•° - Qwen2.5-Coder-1.5B ä¼˜åŒ–
    num_train_epochs: int = 3  # Qwen æ”¶æ•›æ›´å¿«
    per_device_train_batch_size: int = 4
    per_device_eval_batch_size: int = 4
    gradient_accumulation_steps: int = 8  # ç­‰æ•ˆbatch_size = 32
    
    # ä¼˜åŒ–å™¨
    learning_rate: float = 2e-5  # Qwen æ¨èè¾ƒå°å­¦ä¹ ç‡
    weight_decay: float = 0.01
    adam_beta1: float = 0.9
    adam_beta2: float = 0.95  # Qwen æ¨è
    adam_epsilon: float = 1e-8
    max_grad_norm: float = 1.0
    
    # å­¦ä¹ ç‡è°ƒåº¦
    lr_scheduler_type: str = "cosine"
    warmup_ratio: float = 0.1  # ä½¿ç”¨æ¯”ä¾‹è€Œéå›ºå®šæ­¥æ•°
    warmup_steps: int = 0  # å¦‚æœ > 0 åˆ™è¦†ç›– warmup_ratio
    
    # æ—¥å¿—å’Œä¿å­˜
    logging_steps: int = 50
    save_steps: int = 500
    save_total_limit: int = 3
    
    # è¯„ä¼°
    evaluation_strategy: str = "steps"
    eval_steps: int = 500
    load_best_model_at_end: bool = True
    metric_for_best_model: str = "eval_loss"
    
    # æ•°æ®åŠ è½½ - æ ¹æ®ç³»ç»Ÿè‡ªåŠ¨è°ƒæ•´
    # Windows: 2, Linux æœåŠ¡å™¨: 8
    dataloader_num_workers: int = 8 if os.name != 'nt' else 2
    dataloader_pin_memory: bool = True
    
    # æ··åˆç²¾åº¦ - ä¼˜å…ˆä½¿ç”¨ bf16
    fp16: bool = False  # Qwen æ¨èä½¿ç”¨ bf16
    bf16: bool = True   # Ampere+ GPU æ”¯æŒ
    fp16_opt_level: str = "O2"
    
    # ä¼˜åŒ–å™¨é€‰æ‹©
    optim: str = "adamw_torch"  # æˆ– "adamw_bnb_8bit" èŠ‚çœæ˜¾å­˜
    
    # éšæœºç§å­
    seed: int = 42
    
    @classmethod
    def from_model_preset(cls, preset_name: str, attack_type: str = "sqli"):
        """æ ¹æ®æ¨¡å‹é¢„è®¾åˆ›å»ºè®­ç»ƒé…ç½®"""
        if preset_name not in MODEL_PRESETS:
            preset_name = DEFAULT_MODEL
        
        preset = MODEL_PRESETS[preset_name]
        
        return cls(
            output_dir=f"models/pretrain_{attack_type}_{preset_name.replace('.', '_').replace('-', '_')}",
            per_device_train_batch_size=preset.get("batch_size", 4),
            gradient_accumulation_steps=preset.get("gradient_accumulation", 8),
        )


@dataclass
class PPOConfig:
    """PPOå¼ºåŒ–å­¦ä¹ é…ç½® - é’ˆå¯¹8GBæ˜¾å­˜ä¼˜åŒ–ï¼Œæ”¯æŒ Qwen2.5-Coder"""
    # æ¨¡å‹è·¯å¾„
    model_name: str = "models/pretrain_sqli_qwen"
    ref_model_name: Optional[str] = None
    
    # Batché…ç½® - Qwen 1.5B ä¼˜åŒ–
    batch_size: int = 128  # æ¯è½®ç”Ÿæˆçš„æ ·æœ¬æ•°
    mini_batch_size: int = 8  # PPOæ›´æ–°çš„mini batch
    gradient_accumulation_steps: int = 4
    
    # PPOè¶…å‚æ•° (è®ºæ–‡æ¨è)
    learning_rate: float = 1.4e-5
    ppo_epochs: int = 4
    
    # KLæ•£åº¦çº¦æŸ (å…³é”®!)
    init_kl_coef: float = 0.2  # Betaå‚æ•°
    target_kl: float = 0.1
    adap_kl_ctrl: bool = False  # è®ºæ–‡ä½¿ç”¨å›ºå®šbeta
    
    # å¥–åŠ±é…ç½®
    gamma: float = 0.99
    lam: float = 0.95
    
    # ä¼˜åŒ–
    optimize_cuda_cache: bool = True
    max_grad_norm: float = 1.0
    
    # ç”Ÿæˆé…ç½®
    max_new_tokens: int = 128
    temperature: float = 1.0
    top_k: int = 50
    top_p: float = 0.95
    
    # æ—¥å¿—
    log_with: Optional[str] = None  # "tensorboard" æˆ– "wandb"
    seed: int = 42
    
    # æ›´æ–°ç­–ç•¥
    update_batch_size: int = 2  # ç­–ç•¥æ›´æ–°å°æ‰¹é‡


@dataclass
class DataConfig:
    """æ•°æ®é…ç½®"""
    # æ•°æ®è·¯å¾„
    data_dir: str = "gptfuzzer-main/Datasets"
    grammar_dir: str = "gptfuzzer-main/grammar"
    
    # æ•°æ®é›†åˆ†å‰²
    train_split: float = 0.8
    val_split: float = 0.1
    test_split: float = 0.1
    
    # å¤„ç†åçš„æ•°æ®ä¿å­˜è·¯å¾„
    processed_dir: str = "data/processed"
    
    # æ”»å‡»ç±»å‹
    attack_types: List[str] = field(default_factory=lambda: ["sqli", "xss", "rce"])


@dataclass
class WAFConfig:
    """WAFç¯å¢ƒé…ç½®"""
    # WAFç±»å‹
    waf_type: str = "modsecurity"  # modsecurity æˆ– naxsi
    
    # WAF URL
    modsecurity_url: str = "http://localhost:8001"
    naxsi_url: str = "http://localhost:8002"
    
    # è¯·æ±‚é…ç½®
    timeout: int = 10
    max_retries: int = 3
    
    # è¯·æ±‚é¢„ç®—
    request_budget: int = 5000  # æ¯æ¬¡è®­ç»ƒçš„æœ€å¤§WAFè¯·æ±‚æ¬¡æ•°


# åˆ›å»ºé»˜è®¤é…ç½®å®ä¾‹
def get_default_configs(model_preset: str = DEFAULT_MODEL):
    """è·å–é»˜è®¤é…ç½®"""
    model_config = ModelConfig.from_preset(model_preset)
    preset = MODEL_PRESETS.get(model_preset, MODEL_PRESETS[DEFAULT_MODEL])
    
    gpu_config = GPUConfig(
        load_in_4bit=preset.get("load_in_4bit", False),
        use_flash_attention=preset.get("use_flash_attention", True),
    )
    
    return {
        "gpu": gpu_config,
        "model": model_config,
        "training": TrainingConfig.from_model_preset(model_preset),
        "ppo": PPOConfig(),
        "data": DataConfig(),
        "waf": WAFConfig()
    }


def get_quantization_config(gpu_config: GPUConfig):
    """è·å–é‡åŒ–é…ç½® (ç”¨äº bitsandbytes)"""
    if not gpu_config.load_in_4bit and not gpu_config.load_in_8bit:
        return None
    
    try:
        from transformers import BitsAndBytesConfig
        import torch
        
        if gpu_config.load_in_4bit:
            compute_dtype = getattr(torch, gpu_config.bnb_4bit_compute_dtype)
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type=gpu_config.bnb_4bit_quant_type,
                bnb_4bit_compute_dtype=compute_dtype,
                bnb_4bit_use_double_quant=True,
            )
        elif gpu_config.load_in_8bit:
            return BitsAndBytesConfig(load_in_8bit=True)
    except ImportError:
        print("è­¦å‘Š: bitsandbytes æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨é‡åŒ–")
        return None
    
    return None


if __name__ == "__main__":
    # æ‰“å°é…ç½®ç¤ºä¾‹
    print("="*60)
    print("å¯ç”¨æ¨¡å‹é¢„è®¾:")
    print("="*60)
    for name, preset in MODEL_PRESETS.items():
        quant = " (4-bit)" if preset.get("load_in_4bit") else ""
        print(f"  - {name}: {preset['model_name']}{quant}")
    
    print(f"\né»˜è®¤æ¨¡å‹: {DEFAULT_MODEL}")
    
    configs = get_default_configs()
    
    print("\n" + "="*60)
    print(f"é»˜è®¤è®­ç»ƒé…ç½® (RTX 4070 8GB) - {DEFAULT_MODEL}")
    print("="*60)
    
    print("\nğŸ“Š GPUé…ç½®:")
    print(f"  - æ˜¾å­˜é™åˆ¶: {configs['gpu'].total_memory_gb} GB")
    print(f"  - FP16: {configs['gpu'].use_fp16}")
    print(f"  - BF16: {configs['gpu'].use_bf16}")
    print(f"  - 4-bité‡åŒ–: {configs['gpu'].load_in_4bit}")
    print(f"  - Flash Attention: {configs['gpu'].use_flash_attention}")
    
    print("\nğŸ¤– æ¨¡å‹é…ç½®:")
    print(f"  - æ¨¡å‹: {configs['model'].model_name}")
    print(f"  - é¢„è®¾: {configs['model'].model_preset}")
    print(f"  - æœ€å¤§é•¿åº¦: {configs['model'].max_length}")
    
    print("\nğŸ“ è®­ç»ƒé…ç½®:")
    print(f"  - Batch size: {configs['training'].per_device_train_batch_size}")
    print(f"  - Accumulation: {configs['training'].gradient_accumulation_steps}")
    print(f"  - ç­‰æ•ˆbatch size: {configs['training'].per_device_train_batch_size * configs['training'].gradient_accumulation_steps}")
    print(f"  - å­¦ä¹ ç‡: {configs['training'].learning_rate}")
    print(f"  - Epochs: {configs['training'].num_train_epochs}")
    print(f"  - BF16: {configs['training'].bf16}")
    
    print("\nğŸ”„ PPOé…ç½®:")
    print(f"  - Batch size: {configs['ppo'].batch_size}")
    print(f"  - Mini batch: {configs['ppo'].mini_batch_size}")
    print(f"  - KLç³»æ•°: {configs['ppo'].init_kl_coef}")
    print(f"  - å­¦ä¹ ç‡: {configs['ppo'].learning_rate}")
