"""
GPUæ˜¾å­˜ç›‘æ§å·¥å…· - é’ˆå¯¹RTX 4070 8GBä¼˜åŒ–
å®æ—¶ç›‘æ§æ˜¾å­˜ä½¿ç”¨ï¼Œé˜²æ­¢OOM
"""
import torch
import psutil
import os
from datetime import datetime


class GPUMonitor:
    """GPUå’Œç³»ç»Ÿèµ„æºç›‘æ§å™¨"""
    
    def __init__(self, threshold_gb=7.5):
        """
        åˆå§‹åŒ–ç›‘æ§å™¨
        
        Args:
            threshold_gb: æ˜¾å­˜è­¦å‘Šé˜ˆå€¼(GB)ï¼Œé»˜è®¤7.5GB
        """
        self.threshold_gb = threshold_gb
        self.cuda_available = torch.cuda.is_available()
        
        if self.cuda_available:
            self.device_name = torch.cuda.get_device_name(0)
            self.total_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"âœ… GPUæ£€æµ‹æˆåŠŸ: {self.device_name}")
            print(f"âœ… æ˜¾å­˜æ€»é‡: {self.total_memory_gb:.2f} GB")
        else:
            print("âš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
    
    def get_gpu_memory(self):
        """è·å–GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        if not self.cuda_available:
            return None
        
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        free = self.total_memory_gb - reserved
        
        return {
            "allocated": allocated,
            "reserved": reserved,
            "free": free,
            "total": self.total_memory_gb,
            "percent": (reserved / self.total_memory_gb) * 100
        }
    
    def get_system_memory(self):
        """è·å–ç³»ç»Ÿå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        ram = psutil.virtual_memory()
        return {
            "used": ram.used / 1024**3,
            "total": ram.total / 1024**3,
            "percent": ram.percent
        }
    
    def check_and_report(self, step=None, force_gc=False):
        """
        æ£€æŸ¥èµ„æºä½¿ç”¨å¹¶æŠ¥å‘Š
        
        Args:
            step: å½“å‰è®­ç»ƒæ­¥æ•°
            force_gc: æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œåƒåœ¾å›æ”¶
            
        Returns:
            bool: æ˜¯å¦è¶…è¿‡è­¦å‘Šé˜ˆå€¼
        """
        gpu_mem = self.get_gpu_memory()
        sys_mem = self.get_system_memory()
        
        timestamp = datetime.now().strftime("%H:%M:%S")
        step_info = f"Step {step}" if step is not None else "Current"
        
        print(f"\n{'='*60}")
        print(f"[{timestamp}] {step_info} - èµ„æºç›‘æ§")
        print(f"{'='*60}")
        
        if gpu_mem:
            print(f"ğŸ–¥ï¸  GPUæ˜¾å­˜:")
            print(f"   - å·²åˆ†é…: {gpu_mem['allocated']:.2f} GB")
            print(f"   - å·²ä¿ç•™: {gpu_mem['reserved']:.2f} GB")
            print(f"   - ç©ºé—²:   {gpu_mem['free']:.2f} GB")
            print(f"   - ä½¿ç”¨ç‡: {gpu_mem['percent']:.1f}%")
            
            # è­¦å‘Šæ£€æŸ¥
            if gpu_mem['reserved'] > self.threshold_gb:
                print(f"âš ï¸  è­¦å‘Š: æ˜¾å­˜ä½¿ç”¨ {gpu_mem['reserved']:.2f} GB è¶…è¿‡é˜ˆå€¼ {self.threshold_gb} GB!")
                print(f"   å»ºè®®: å‡å°batch_sizeæˆ–å¯ç”¨æ›´å¤šä¼˜åŒ–é€‰é¡¹")
                return True
        
        print(f"ğŸ’¾ ç³»ç»Ÿå†…å­˜:")
        print(f"   - å·²ä½¿ç”¨: {sys_mem['used']:.2f} GB / {sys_mem['total']:.2f} GB")
        print(f"   - ä½¿ç”¨ç‡: {sys_mem['percent']:.1f}%")
        
        if force_gc or (gpu_mem and gpu_mem['reserved'] > self.threshold_gb * 0.9):
            self.clear_cache()
        
        return False
    
    def clear_cache(self):
        """æ¸…ç†GPUç¼“å­˜"""
        if self.cuda_available:
            torch.cuda.empty_cache()
            print("ğŸ§¹ å·²æ¸…ç†GPUç¼“å­˜")
    
    def get_recommended_batch_size(self):
        """æ ¹æ®æ˜¾å­˜æƒ…å†µæ¨èbatch size"""
        if not self.cuda_available:
            return 1
        
        gpu_mem = self.get_gpu_memory()
        free_gb = gpu_mem['free']
        
        if free_gb > 6:
            return 4
        elif free_gb > 4:
            return 2
        else:
            return 1


def test_gpu_setup():
    """æµ‹è¯•GPUé…ç½®"""
    print("\n" + "="*60)
    print("GPUé…ç½®æµ‹è¯•")
    print("="*60)
    
    monitor = GPUMonitor()
    
    if not monitor.cuda_available:
        print("âŒ é”™è¯¯: æœªæ£€æµ‹åˆ°CUDA")
        return False
    
    print(f"\nğŸ“Š CUDAä¿¡æ¯:")
    print(f"   - CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"   - PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   - cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    
    # æµ‹è¯•å°æ¨¡å‹åŠ è½½
    try:
        print(f"\nğŸ§ª æµ‹è¯•åŠ è½½GPT-2 Small...")
        from transformers import GPT2LMHeadModel
        
        model = GPT2LMHeadModel.from_pretrained("gpt2")
        model = model.to("cuda")
        
        monitor.check_and_report()
        
        del model
        torch.cuda.empty_cache()
        
        print(f"\nâœ… GPUé…ç½®æµ‹è¯•é€šè¿‡!")
        return True
        
    except Exception as e:
        print(f"\nâŒ GPUæµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    success = test_gpu_setup()
    
    if success:
        print("\n" + "="*60)
        print("å»ºè®®çš„è®­ç»ƒé…ç½®:")
        print("="*60)
        monitor = GPUMonitor()
        recommended_bs = monitor.get_recommended_batch_size()
        print(f"âœ… æ¨è batch_size: {recommended_bs}")
        print(f"âœ… æ¨è gradient_accumulation_steps: {32 // recommended_bs}")
        print(f"âœ… å¿…é¡»å¯ç”¨: fp16=True, gradient_checkpointing=True")
        print(f"âœ… å¿…é¡»ä½¿ç”¨: optim='adamw_bnb_8bit'")
