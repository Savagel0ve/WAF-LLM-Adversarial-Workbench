"""
GPTFuzzer Stage 3: Reinforcement Learning (PPO)
ä½¿ç”¨PPOç®—æ³•å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆèƒ½ç»•è¿‡WAFçš„è½½è·

æ”¯æŒçš„æ¨¡å‹:
- Qwen2.5-Coder-0.5B/1.5B/3B (æ¨è)
- DeepSeek-Coder
- Phi-3
- GPT-2 (å…¼å®¹)

å‚è€ƒè®ºæ–‡:
- GPTFuzzer: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts
- Proximal Policy Optimization (PPO): https://arxiv.org/abs/1707.06347

æ ¸å¿ƒæ€è·¯:
1. Policy Network: ä»é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–ï¼Œç”¨äºç”Ÿæˆè½½è·
2. Reference Model: å†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œç”¨äºè®¡ç®—KLæ•£åº¦
3. Reward Model: è®­ç»ƒå¥½çš„åˆ†ç±»å™¨ï¼Œè¯„ä¼°è½½è·ç»•è¿‡WAFçš„æ¦‚ç‡
4. PPOç®—æ³•: åœ¨å¥–åŠ±å’ŒKLæ•£åº¦ä¹‹é—´å¹³è¡¡ï¼Œä¼˜åŒ–ç­–ç•¥ç½‘ç»œ
"""

import os
import sys
import json
import torch
import argparse
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import numpy as np
from tqdm import tqdm

# Transformerså’ŒTRL
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    pipeline
)
from trl import (
    PPOTrainer,
    PPOConfig,
    AutoModelForCausalLMWithValueHead,
    create_reference_model
)

# æœ¬åœ°å¯¼å…¥
try:
    from config import get_default_configs, MODEL_PRESETS, DEFAULT_MODEL
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥config.pyï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
    get_default_configs = None
    MODEL_PRESETS = {}
    DEFAULT_MODEL = "qwen2.5-coder-1.5b"


class RLConfig:
    """å¼ºåŒ–å­¦ä¹ é…ç½®ç±» - æ”¯æŒ Qwen2.5-Coder å’Œå…¶ä»–ç°ä»£ LLM"""
    def __init__(
        self,
        # æ¨¡å‹è·¯å¾„
        pretrained_model_path: str = "./models/pretrain_sqli_qwen2_5_coder_1_5b",
        reward_model_path: str = "./models/reward_sqli_qwen/final_reward_model",
        output_dir: str = "./models/rl_sqli_qwen",
        
        # PPOè¶…å‚æ•° (æ¥è‡ªè®ºæ–‡)
        learning_rate: float = 1.4e-5,
        batch_size: int = 128,  # Qwen 1.5B ä¼˜åŒ–
        mini_batch_size: int = 8,
        ppo_epochs: int = 4,
        init_kl_coef: float = 0.2,  # Betaå‚æ•°ï¼Œå…³é”®!
        target_kl: float = 0.1,
        adap_kl_ctrl: bool = False,  # è®ºæ–‡ä½¿ç”¨å›ºå®šbeta
        
        # ç”Ÿæˆé…ç½®
        max_new_tokens: int = 128,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.95,
        start_prompt: str = "SELECT",  # SQL payload æ›´å¥½çš„èµ·å§‹
        
        # è®­ç»ƒé…ç½®
        total_episodes: int = 20,
        save_freq: int = 5,
        update_batch_size: int = 2,
        
        # ç²¾åº¦é…ç½®
        use_fp16: bool = False,
        use_bf16: bool = True,  # Qwen æ¨è bf16
        gradient_accumulation_steps: int = 1,
        
        # å…¶ä»–
        seed: int = 42,
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
        log_with: Optional[str] = None,  # "wandb" or "tensorboard"
    ):
        self.pretrained_model_path = pretrained_model_path
        self.reward_model_path = reward_model_path
        self.output_dir = output_dir
        
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.mini_batch_size = mini_batch_size
        self.ppo_epochs = ppo_epochs
        self.init_kl_coef = init_kl_coef
        self.target_kl = target_kl
        self.adap_kl_ctrl = adap_kl_ctrl
        
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.top_k = top_k
        self.top_p = top_p
        self.start_prompt = start_prompt
        
        self.total_episodes = total_episodes
        self.save_freq = save_freq
        self.update_batch_size = update_batch_size
        
        self.use_fp16 = use_fp16
        self.use_bf16 = use_bf16
        self.gradient_accumulation_steps = gradient_accumulation_steps
        
        self.seed = seed
        self.device = device
        self.log_with = log_with
        
        # è®¡ç®—å®é™…çš„ mini_batch æ•°é‡
        self.gradient_accumulation_steps = max(1, batch_size // mini_batch_size)
        
    def to_ppo_config(self) -> PPOConfig:
        """è½¬æ¢ä¸ºTRLçš„PPOConfig"""
        return PPOConfig(
            output_dir=self.output_dir,
            learning_rate=self.learning_rate,
            batch_size=self.batch_size,
            mini_batch_size=self.mini_batch_size,
            gradient_accumulation_steps=self.gradient_accumulation_steps,
            num_ppo_epochs=self.ppo_epochs,
            kl_coef=self.init_kl_coef,
            seed=self.seed,
            log_with=self.log_with if self.log_with else None,
            logging_steps=10,
            save_strategy="no",  # æˆ‘ä»¬æ‰‹åŠ¨ä¿å­˜
        )
    
    def save(self, path: str):
        """ä¿å­˜é…ç½®åˆ°JSON"""
        config_dict = {k: v for k, v in self.__dict__.items() if not k.startswith('_')}
        with open(path, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
    
    @classmethod
    def load(cls, path: str):
        """ä»JSONåŠ è½½é…ç½®"""
        with open(path, 'r', encoding='utf-8') as f:
            config_dict = json.load(f)
        return cls(**config_dict)


class RewardModelWrapper:
    """å¥–åŠ±æ¨¡å‹åŒ…è£…å™¨ - å°†åˆ†ç±»å™¨è¾“å‡ºè½¬æ¢ä¸ºå¥–åŠ±ä¿¡å·"""
    
    def __init__(self, model_path: str, tokenizer, device: str = "cuda"):
        """
        åˆå§‹åŒ–å¥–åŠ±æ¨¡å‹
        
        Args:
            model_path: å¥–åŠ±æ¨¡å‹è·¯å¾„
            tokenizer: tokenizerå®ä¾‹
            device: è®¾å¤‡
        """
        self.device = device
        
        # åŠ è½½å¥–åŠ±æ¨¡å‹
        print(f"ğŸ åŠ è½½å¥–åŠ±æ¨¡å‹: {model_path}")
        
        # å…ˆåŠ è½½æ¨¡å‹é…ç½®ï¼Œä¸è¦†ç›–num_labels
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_path,
            device_map=device,
            ignore_mismatched_sizes=True  # å¿½ç•¥å¤§å°ä¸åŒ¹é…è­¦å‘Š
        )
        self.model.eval()
        
        # æ£€æµ‹æ¨¡å‹çš„æ ‡ç­¾æ•°é‡
        self.num_labels = self.model.config.num_labels
        print(f"   æ£€æµ‹åˆ° {self.num_labels} ä¸ªæ ‡ç­¾")
        
        self.tokenizer = tokenizer
        
        # æ ¹æ®æ¨¡å‹ç±»å‹åˆ›å»ºpipeline
        if self.num_labels == 1:
            # BCEWithLogitsLosså½¢å¼ï¼Œä¸ä½¿ç”¨pipelineè€Œç›´æ¥æ¨ç†
            self.pipe = None
        else:
            # æ ‡å‡†åˆ†ç±»ï¼Œä½¿ç”¨pipeline
            self.pipe = pipeline(
                "text-classification",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if device == "cuda" else -1,
                return_all_scores=True
            )
    
    def get_rewards(self, texts: List[str]) -> List[float]:
        """
        è®¡ç®—ä¸€æ‰¹æ–‡æœ¬çš„å¥–åŠ±
        
        Args:
            texts: ç”Ÿæˆçš„è½½è·åˆ—è¡¨
            
        Returns:
            å¥–åŠ±åˆ—è¡¨ (0-1ä¹‹é—´çš„æµ®ç‚¹æ•°)
        """
        try:
            if self.num_labels == 1:
                # BCEWithLogitsLosså½¢å¼ï¼šç›´æ¥æ¨ç†è·å–logitsç„¶åsigmoid
                # ç¡®ä¿ä½¿ç”¨left padding
                original_padding_side = self.tokenizer.padding_side
                self.tokenizer.padding_side = 'left'
                
                inputs = self.tokenizer(
                    texts,
                    padding=True,
                    truncation=True,
                    max_length=512,
                    return_tensors="pt"
                ).to(self.device)
                
                # æ¢å¤åŸæ¥çš„padding side
                self.tokenizer.padding_side = original_padding_side
                
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    logits = outputs.logits.squeeze(-1)  # [batch_size, 1] -> [batch_size]
                    # åº”ç”¨sigmoidå°†logitsè½¬æ¢ä¸ºæ¦‚ç‡
                    probs = torch.sigmoid(logits)
                    rewards = probs.cpu().tolist()
                
                return rewards
            
            else:
                # æ ‡å‡†äºŒåˆ†ç±»ï¼šä½¿ç”¨pipeline
                outputs = self.pipe(texts)
                
                # æå–å¥–åŠ±åˆ†æ•°
                rewards = []
                for output in outputs:
                    # outputæ˜¯ä¸€ä¸ªåŒ…å«å­—å…¸çš„åˆ—è¡¨
                    if isinstance(output, list) and len(output) > 0:
                        # æ‰¾åˆ°LABEL_1çš„åˆ†æ•°
                        label1_score = None
                        for item in output:
                            if item.get('label') in ['LABEL_1', '1', 1]:
                                label1_score = item['score']
                                break
                        
                        if label1_score is not None:
                            rewards.append(label1_score)
                        elif len(output) >= 2:
                            # å¦‚æœæ‰¾ä¸åˆ°LABEL_1ï¼Œä½¿ç”¨ç¬¬äºŒä¸ªå…ƒç´ 
                            rewards.append(output[1]['score'])
                        else:
                            rewards.append(0.0)
                    else:
                        # é™çº§å¤„ç†
                        rewards.append(0.0)
                
                return rewards
        
        except Exception as e:
            print(f"âš ï¸  å¥–åŠ±è®¡ç®—å‡ºé”™: {e}")
            print(f"   æ–‡æœ¬æ ·ä¾‹: {texts[0] if texts else 'None'}")
            import traceback
            traceback.print_exc()
            # è¿”å›é›¶å¥–åŠ±
            return [0.0] * len(texts)
    
    def __call__(self, texts: List[str]) -> List[torch.Tensor]:
        """
        ä½¿æ¥å£å…¼å®¹PPOTrainer
        
        Returns:
            torch.Tensoråˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯æ ‡é‡å¥–åŠ±
        """
        rewards = self.get_rewards(texts)
        return [torch.tensor(r, dtype=torch.float32) for r in rewards]


class RLTrainer:
    """å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨ - å°è£…PPOè®­ç»ƒé€»è¾‘"""
    
    def __init__(self, config: RLConfig):
        """
        åˆå§‹åŒ–RLè®­ç»ƒå™¨
        
        Args:
            config: RLConfigé…ç½®å¯¹è±¡
        """
        self.config = config
        
        # è®¾ç½®éšæœºç§å­
        self._set_seed(config.seed)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(config.output_dir, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        config.save(os.path.join(config.output_dir, "rl_config.json"))
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œtrainer
        self.tokenizer = None
        self.model = None
        self.ref_model = None
        self.reward_model = None
        self.ppo_trainer = None
        
        self._setup_models()
    
    def _set_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­"""
        import random
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
    
    def _setup_models(self):
        """è®¾ç½®æ‰€æœ‰éœ€è¦çš„æ¨¡å‹ - æ”¯æŒ Qwen2.5-Coder å’Œå…¶ä»–ç°ä»£ LLM"""
        print("\n" + "="*60)
        print("ğŸš€ åˆå§‹åŒ–å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ")
        print("="*60)
        
        # 1. åŠ è½½ Tokenizer (ä½¿ç”¨ AutoTokenizer æ”¯æŒå„ç§æ¨¡å‹)
        print(f"\nğŸ“ åŠ è½½ Tokenizer: {self.config.pretrained_model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.pretrained_model_path,
            trust_remote_code=True,
            padding_side='left'  # Causal LM ç”Ÿæˆéœ€è¦ left padding
        )
        
        # è®¾ç½® pad token
        if self.tokenizer.pad_token is None:
            if self.tokenizer.eos_token:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            else:
                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        
        self.tokenizer.padding_side = 'left'
        print(f"   Tokenizer: {type(self.tokenizer).__name__}")
        print(f"   Padding side: {self.tokenizer.padding_side}")
        print(f"   Vocab size: {len(self.tokenizer)}")

        # è®¾ç½®èµ·å§‹ prompt
        prompt_ids = self.tokenizer.encode(self.config.start_prompt, add_special_tokens=False)
        if not prompt_ids or all(token_id == self.tokenizer.pad_token_id for token_id in prompt_ids):
            # å°è¯•ä½¿ç”¨æ•°å­—ä½œä¸º fallback
            fallback_prompt = "1"
            prompt_ids = self.tokenizer.encode(fallback_prompt, add_special_tokens=False)
            if not prompt_ids:
                # ä½¿ç”¨ BOS token å¦‚æœå­˜åœ¨ï¼Œå¦åˆ™ç”¨ EOS
                if self.tokenizer.bos_token_id is not None:
                    prompt_ids = [self.tokenizer.bos_token_id]
                else:
                    prompt_ids = [self.tokenizer.eos_token_id]
            print(f"   âš ï¸ start_prompt æ— æ•ˆï¼Œä½¿ç”¨ fallback")
        self.prompt_ids = prompt_ids
        print(f"   Start prompt ids: {self.prompt_ids}")
        
        # 2. ç¡®å®šç²¾åº¦
        if self.config.use_bf16:
            torch_dtype = torch.bfloat16
        elif self.config.use_fp16:
            torch_dtype = torch.float16
        else:
            torch_dtype = torch.float32
        
        # 3. åŠ è½½ Policy Network (å¸¦ Value Head)
        print(f"\nğŸ¤– åŠ è½½ Policy Network (å¸¦ Value Head)")
        print(f"   ç²¾åº¦: {torch_dtype}")
        
        self.model = AutoModelForCausalLMWithValueHead.from_pretrained(
            self.config.pretrained_model_path,
            trust_remote_code=True,
            torch_dtype=torch_dtype,
        )
        self.model.to(self.config.device)
        
        # å¦‚æœæ·»åŠ äº†æ–°çš„ pad tokenï¼Œè°ƒæ•´ embedding
        if len(self.tokenizer) > self.model.pretrained_model.config.vocab_size:
            self.model.pretrained_model.resize_token_embeddings(len(self.tokenizer))
        
        # 4. åˆ›å»º Reference Model (å†»ç»“å‚æ•°)
        print(f"\nğŸ”’ åˆ›å»º Reference Model (å†»ç»“å‚æ•°)")
        self.ref_model = create_reference_model(self.model)
        
        # 5. åŠ è½½ Reward Model
        print(f"\nğŸ åŠ è½½ Reward Model: {self.config.reward_model_path}")
        self.reward_model = RewardModelWrapper(
            self.config.reward_model_path,
            self.tokenizer,
            self.config.device
        )
        
        print("\nâœ… æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ!")
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"   - Policy Model å‚æ•°: {total_params / 1e9:.2f}B ({total_params / 1e6:.1f}M)")
        print(f"   - å¯è®­ç»ƒå‚æ•°: {trainable_params / 1e9:.2f}B")
        print(f"   - è®¾å¤‡: {self.config.device}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {self.config.batch_size}")
        print(f"   - Mini æ‰¹æ¬¡å¤§å°: {self.config.mini_batch_size}")
        print(f"   - KL ç³»æ•° (Î²): {self.config.init_kl_coef}")
    
    def generate_queries(self, batch_size: int) -> List[torch.Tensor]:
        """
        ç”ŸæˆæŸ¥è¯¢ (èµ·å§‹token)
        
        åœ¨GPTFuzzerä¸­ï¼Œé€šå¸¸ä½¿ç”¨<start>æˆ–BOS tokenä½œä¸ºèµ·å§‹
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            query tensoråˆ—è¡¨
        """
        # ä½¿ç”¨BOS tokenä½œä¸ºèµ·å§‹
        query_tensors = []
        for _ in range(batch_size):
            # åˆ›å»ºåªåŒ…å«BOS tokençš„è¾“å…¥
            query_tensor = torch.tensor([[self.tokenizer.bos_token_id]], dtype=torch.long)
            query_tensors.append(query_tensor.squeeze(0))
        
        return query_tensors
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯ - ç®€åŒ–ç‰ˆPPOå®ç°"""
        print("\n" + "="*60)
        print("ğŸ¯ å¼€å§‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
        print("="*60)
        print(f"æ€»è®­ç»ƒè½®æ•°: {self.config.total_episodes}")
        print(f"æ¯è½®ç”Ÿæˆ: {self.config.batch_size} ä¸ªè½½è·")
        print(f"é¢„è®¡ç”Ÿæˆæ€»æ•°: {self.config.total_episodes * self.config.batch_size}")
        print("="*60 + "\n")
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.learning_rate)
        scaler = torch.amp.GradScaler("cuda", enabled=self.config.use_fp16 and self.config.device == "cuda")
        
        # è®­ç»ƒç»Ÿè®¡
        all_rewards = []
        
        # ä¸»è®­ç»ƒå¾ªç¯
        for episode in range(self.config.total_episodes):
            print(f"\n{'='*60}")
            print(f"Episode {episode + 1}/{self.config.total_episodes}")
            print(f"{'='*60}")
            
            # æ¸…ç†ç¼“å­˜ï¼Œé¿å…ç´¯è®¡æ˜¾å­˜å ç”¨
            if self.config.device == "cuda":
                torch.cuda.empty_cache()

            # === Step 1: Generate (Rollout) ===
            print(f"\nğŸ² ç”Ÿæˆ {self.config.batch_size} ä¸ªè½½è·...")
            
            batch_texts = []
            batch_rewards = []
            
            # åˆ†æ‰¹ç”Ÿæˆä»¥èŠ‚çœæ˜¾å­˜
            num_batches = (self.config.batch_size + self.config.mini_batch_size - 1) // self.config.mini_batch_size
            
            for batch_idx in range(num_batches):
                current_batch_size = min(
                    self.config.mini_batch_size,
                    self.config.batch_size - len(batch_texts)
                )
                
                # åˆ›å»ºèµ·å§‹è¾“å…¥å¹¶è®¾ç½®attention mask
                prompt_tensor = torch.tensor(self.prompt_ids, dtype=torch.long, device=self.config.device)
                input_ids = prompt_tensor.unsqueeze(0).repeat(current_batch_size, 1)
                attention_mask = torch.ones_like(input_ids)
                
                # ç”Ÿæˆ
                with torch.no_grad():
                    outputs = self.model.generate(
                        input_ids,
                        attention_mask=attention_mask,
                        max_new_tokens=self.config.max_new_tokens,
                        temperature=self.config.temperature,
                        top_k=self.config.top_k,
                        top_p=self.config.top_p,
                        do_sample=True,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                    )
                
                # è§£ç 
                for output in outputs:
                    text = self.tokenizer.decode(output, skip_special_tokens=True)
                    batch_texts.append(text)
            
            # æ˜¾ç¤ºæ ·ä¾‹
            print(f"\nğŸ“„ ç”Ÿæˆæ ·ä¾‹ (å‰3ä¸ª):")
            for i, text in enumerate(batch_texts[:3]):
                print(f"   [{i+1}] {text[:100]}..." if len(text) > 100 else f"   [{i+1}] {text}")
            
            # === Step 2: Calculate Rewards ===
            print(f"\nğŸ è®¡ç®—å¥–åŠ±...")
            
            rewards = self.reward_model.get_rewards(batch_texts)
            
            # ç»Ÿè®¡å¥–åŠ±
            mean_reward = np.mean(rewards)
            max_reward = np.max(rewards)
            min_reward = np.min(rewards)
            
            all_rewards.extend(rewards)
            
            print(f"\nğŸ“Š å¥–åŠ±ç»Ÿè®¡:")
            print(f"   - å¹³å‡å¥–åŠ±: {mean_reward:.4f}")
            print(f"   - æœ€å¤§å¥–åŠ±: {max_reward:.4f}")
            print(f"   - æœ€å°å¥–åŠ±: {min_reward:.4f}")
            
            # === Step 3: Simple Policy Update ===
            # ç®€åŒ–ç‰ˆï¼šåŸºäºREINFORCEç®—æ³•æ›´æ–°ç­–ç•¥
            print(f"\nğŸ”„ æ‰§è¡Œç­–ç•¥æ›´æ–°...")
            
            try:
                optimizer.zero_grad(set_to_none=True)
                if self.config.device == "cuda":
                    torch.cuda.empty_cache()
                
                # é‡æ–°è®¡ç®—ç”Ÿæˆçš„logæ¦‚ç‡
                total_loss = 0
                num_samples = 0
                
                # ç¡®ä¿ä½¿ç”¨left padding
                original_padding_side = self.tokenizer.padding_side
                self.tokenizer.padding_side = 'left'
                
                update_batch_size = max(1, self.config.update_batch_size)
                for start_idx in range(0, len(batch_texts), update_batch_size):
                    batch_slice = batch_texts[start_idx:start_idx + update_batch_size]
                    batch_rewards = rewards[start_idx:start_idx + update_batch_size]

                    # Tokenize batch
                    inputs = self.tokenizer(
                        batch_slice,
                        return_tensors="pt",
                        truncation=True,
                        max_length=self.config.max_new_tokens,
                        padding=True
                    ).to(self.config.device)

                    with torch.amp.autocast(device_type="cuda", enabled=self.config.use_fp16 and self.config.device == "cuda"):
                        # å‰å‘ä¼ æ’­
                        outputs = self.model(**inputs)

                        # è®¡ç®—è¯­è¨€æ¨¡å‹lossï¼ˆæ‰‹åŠ¨ï¼‰
                        logits = outputs.logits if hasattr(outputs, "logits") else outputs[0]
                        labels = inputs["input_ids"]
                        # shift for causal LM
                        shift_logits = logits[:, :-1, :].contiguous()
                        shift_labels = labels[:, 1:].contiguous()
                        # è®¡ç®—äº¤å‰ç†µ
                        model_loss = torch.nn.functional.cross_entropy(
                            shift_logits.view(-1, shift_logits.size(-1)),
                            shift_labels.view(-1),
                            ignore_index=self.tokenizer.pad_token_id
                        )

                        # ä½¿ç”¨batchå¹³å‡reward
                        reward_tensor = torch.tensor(batch_rewards, device=self.config.device, dtype=model_loss.dtype)
                        reward_mean = reward_tensor.mean()
                        loss = -model_loss * reward_mean

                    scaler.scale(loss).backward()
                    total_loss += loss.detach()
                    num_samples += len(batch_slice)

                    # é‡Šæ”¾æ˜¾å­˜
                    del inputs, outputs, logits, labels, shift_logits, shift_labels, model_loss, loss, reward_tensor
                    if self.config.device == "cuda":
                        torch.cuda.empty_cache()
                
                # å¹³å‡loss
                avg_loss = total_loss / num_samples
                
                # åå‘ä¼ æ’­
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                scaler.step(optimizer)
                scaler.update()
                
                # æ¢å¤padding side
                self.tokenizer.padding_side = original_padding_side
                
                print(f"   - å¹³å‡æŸå¤±: {avg_loss.item():.4f}")
                
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print("âš ï¸  OOM during policy update, clearing cache and continuing.")
                    if self.config.device == "cuda":
                        torch.cuda.empty_cache()
                    continue
                print(f"âš ï¸  ç­–ç•¥æ›´æ–°å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                continue
            except Exception as e:
                print(f"âš ï¸  ç­–ç•¥æ›´æ–°å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                continue
            
            # === Step 4: Save Checkpoint ===
            if (episode + 1) % self.config.save_freq == 0:
                checkpoint_dir = os.path.join(
                    self.config.output_dir,
                    f"checkpoint-{episode + 1}"
                )
                print(f"\nğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_dir}")
                self.save_model(checkpoint_dir)
            
            # æ‰“å°ç´¯ç§¯ç»Ÿè®¡
            print(f"\nğŸ“ˆ ç´¯ç§¯ç»Ÿè®¡ (Episodes 1-{episode+1}):")
            print(f"   - å¹³å‡å¥–åŠ±: {np.mean(all_rewards):.4f}")
        
        # === Final Save ===
        print(f"\n{'='*60}")
        print("ğŸ‰ è®­ç»ƒå®Œæˆ!")
        print(f"{'='*60}")
        
        final_dir = os.path.join(self.config.output_dir, "final_model")
        print(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_dir}")
        self.save_model(final_dir)
        
        # ä¿å­˜è®­ç»ƒç»Ÿè®¡
        stats_file = os.path.join(self.config.output_dir, "training_stats.json")
        with open(stats_file, 'w') as f:
            json.dump({
                'all_rewards': all_rewards,
                'mean_reward': float(np.mean(all_rewards)),
                'final_reward': float(np.mean(all_rewards[-self.config.batch_size:]) if len(all_rewards) >= self.config.batch_size else np.mean(all_rewards)),
                'total_episodes': self.config.total_episodes,
            }, f, indent=2)
        
        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡å·²ä¿å­˜: {stats_file}")
        print(f"\næœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(all_rewards):.4f}")
        
        return {
            'mean_reward': np.mean(all_rewards),
            'all_rewards': all_rewards,
        }
    
    def save_model(self, output_dir: str):
        """ä¿å­˜æ¨¡å‹å’Œtokenizer"""
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜policy model (åªä¿å­˜base modeléƒ¨åˆ†)
        if hasattr(self.model, 'pretrained_model'):
            self.model.pretrained_model.save_pretrained(output_dir)
        else:
            # å¦‚æœæ˜¯æ™®é€šæ¨¡å‹ï¼Œç›´æ¥ä¿å­˜
            self.model.save_pretrained(output_dir)
        
        # ä¿å­˜tokenizer
        self.tokenizer.save_pretrained(output_dir)
        
        # ä¿å­˜é…ç½®
        self.config.save(os.path.join(output_dir, "rl_config.json"))
        
        print(f"   âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="GPTFuzzer Stage 3: å¼ºåŒ–å­¦ä¹  (PPO) - æ”¯æŒ Qwen2.5-Coder",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨ Qwen2.5-Coder æ¨¡å‹
  python train_rl.py \\
      --pretrained_model ./models/pretrain_sqli_qwen2_5_coder_1_5b \\
      --reward_model ./models/reward_sqli_qwen/final_reward_model \\
      --output_dir ./models/rl_sqli_qwen

  # è°ƒæ•´æ‰¹æ¬¡å¤§å° (æ˜¾å­˜ä¸è¶³æ—¶)
  python train_rl.py \\
      --batch_size 64 --mini_batch_size 4 --update_batch_size 1
        """
    )
    
    # æ¨¡å‹è·¯å¾„
    parser.add_argument(
        "--pretrained_model",
        type=str,
        default="./models/pretrain_sqli_qwen2_5_coder_1_5b",
        help="é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ (Stage 1)"
    )
    parser.add_argument(
        "--reward_model",
        type=str,
        default="./models/reward_sqli_qwen/final_reward_model",
        help="å¥–åŠ±æ¨¡å‹è·¯å¾„ (Stage 2)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./models/rl_sqli_qwen",
        help="è¾“å‡ºç›®å½•"
    )
    
    # è®­ç»ƒè¶…å‚æ•°
    parser.add_argument("--learning_rate", type=float, default=1.4e-5, help="å­¦ä¹ ç‡ (è®ºæ–‡å€¼: 1.4e-5)")
    parser.add_argument("--batch_size", type=int, default=128, help="æ‰¹æ¬¡å¤§å° (Qwen 1.5B: 128)")
    parser.add_argument("--mini_batch_size", type=int, default=8, help="Miniæ‰¹æ¬¡å¤§å°")
    parser.add_argument("--init_kl_coef", type=float, default=0.2, help="KLç³»æ•°Î² (è®ºæ–‡å€¼: 0.2)")
    parser.add_argument("--total_episodes", type=int, default=20, help="æ€»è®­ç»ƒè½®æ•°")
    parser.add_argument("--update_batch_size", type=int, default=2, help="ç­–ç•¥æ›´æ–°çš„å°æ‰¹é‡å¤§å°")
    
    # ç”Ÿæˆé…ç½®
    parser.add_argument("--max_new_tokens", type=int, default=128, help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--temperature", type=float, default=1.0, help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--start_prompt", type=str, default="SELECT", help="ç”Ÿæˆèµ·å§‹prompt")
    
    # ç²¾åº¦é…ç½®
    parser.add_argument("--bf16", action="store_true", default=True, help="ä½¿ç”¨ BF16 ç²¾åº¦ (é»˜è®¤)")
    parser.add_argument("--fp16", action="store_true", help="ä½¿ç”¨ FP16 ç²¾åº¦")
    parser.add_argument("--no-bf16", action="store_true", help="ç¦ç”¨ BF16")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--save_freq", type=int, default=5, help="ä¿å­˜é¢‘ç‡")
    parser.add_argument("--log_with", type=str, default=None, help="æ—¥å¿—å·¥å…· (wandb/tensorboard)")
    
    args = parser.parse_args()
    
    # å¤„ç†ç²¾åº¦å‚æ•°
    use_bf16 = args.bf16 and not args.no_bf16 and not args.fp16
    use_fp16 = args.fp16 or (not use_bf16 and not args.no_bf16)
    
    # åˆ›å»ºé…ç½®
    config = RLConfig(
        pretrained_model_path=args.pretrained_model,
        reward_model_path=args.reward_model,
        output_dir=args.output_dir,
        learning_rate=args.learning_rate,
        batch_size=args.batch_size,
        mini_batch_size=args.mini_batch_size,
        init_kl_coef=args.init_kl_coef,
        total_episodes=args.total_episodes,
        update_batch_size=args.update_batch_size,
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature,
        start_prompt=args.start_prompt,
        use_bf16=use_bf16,
        use_fp16=use_fp16,
        seed=args.seed,
        save_freq=args.save_freq,
        log_with=args.log_with,
    )
    
    # æ‰“å°é…ç½®
    print("\n" + "="*60)
    print("âš™ï¸  è®­ç»ƒé…ç½®")
    print("="*60)
    print(f"é¢„è®­ç»ƒæ¨¡å‹: {config.pretrained_model_path}")
    print(f"å¥–åŠ±æ¨¡å‹: {config.reward_model_path}")
    print(f"è¾“å‡ºç›®å½•: {config.output_dir}")
    print(f"å­¦ä¹ ç‡: {config.learning_rate}")
    print(f"æ‰¹æ¬¡å¤§å°: {config.batch_size}")
    print(f"Miniæ‰¹æ¬¡å¤§å°: {config.mini_batch_size}")
    print(f"KLç³»æ•° (Î²): {config.init_kl_coef}")
    print(f"è®­ç»ƒè½®æ•°: {config.total_episodes}")
    print(f"è®¾å¤‡: {config.device}")
    print("="*60 + "\n")
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config.pretrained_model_path):
        print(f"âŒ é”™è¯¯: é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {config.pretrained_model_path}")
        print(f"   è¯·å…ˆå®ŒæˆStage 1 (é¢„è®­ç»ƒ)")
        sys.exit(1)
    
    if not os.path.exists(config.reward_model_path):
        print(f"âŒ é”™è¯¯: å¥–åŠ±æ¨¡å‹ä¸å­˜åœ¨: {config.reward_model_path}")
        print(f"   è¯·å…ˆå®ŒæˆStage 2 (å¥–åŠ±æ¨¡å‹è®­ç»ƒ)")
        sys.exit(1)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = RLTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    try:
        results = trainer.train()
        
        print("\n" + "="*60)
        print("âœ… è®­ç»ƒæˆåŠŸå®Œæˆ!")
        print("="*60)
        print(f"\næœ€ç»ˆç»“æœ:")
        print(f"  - å¹³å‡å¥–åŠ±: {results['mean_reward']:.4f}")
        print(f"  - æ¨¡å‹ä¿å­˜ä½ç½®: {config.output_dir}")
        print(f"\nä¸‹ä¸€æ­¥: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”ŸæˆWAFç»•è¿‡è½½è·")
        print(f"  python train/test_rl_model.py --model_path {config.output_dir}/final_model")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print("æ­£åœ¨ä¿å­˜å½“å‰æ¨¡å‹...")
        trainer.save_model(os.path.join(config.output_dir, "interrupted_model"))
        print("å·²ä¿å­˜")
    
    except Exception as e:
        print(f"\n\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
