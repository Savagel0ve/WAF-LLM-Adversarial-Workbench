"""
é¢„è®­ç»ƒè„šæœ¬ - æ”¯æŒ Qwen2.5-Coder å’Œå…¶ä»–ç°ä»£ LLM
é’ˆå¯¹ RTX 4070 8GB ä¼˜åŒ–

æ”¯æŒçš„æ¨¡å‹:
- Qwen2.5-Coder-0.5B/1.5B/3B (æ¨è)
- Qwen2.5-0.5B/1.5B
- DeepSeek-Coder-1.3B
- Phi-3-mini
- GPT-2 (å…¼å®¹æ—§ç‰ˆ)
"""
import os
import sys
import torch
import argparse
from pathlib import Path
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    set_seed,
    BitsAndBytesConfig,
)
from config import (
    ModelConfig, 
    TrainingConfig, 
    GPUConfig,
    MODEL_PRESETS,
    DEFAULT_MODEL,
    get_quantization_config,
)


class PayloadDataset:
    """Payloadæ•°æ®é›†åŠ è½½å™¨"""
    
    def __init__(self, data_dir="data/processed", attack_type="xss"):
        self.data_dir = Path(data_dir) / attack_type
        self.attack_type = attack_type
    
    def load(self):
        """åŠ è½½æ•°æ®é›†"""
        data_files = {}
        
        for split, filename in [("train", "train.txt"), ("validation", "val.txt"), ("test", "test.txt")]:
            path = self.data_dir / filename
            if path.exists():
                data_files[split] = str(path)
            else:
                print(f"è­¦å‘Š: {path} ä¸å­˜åœ¨ï¼Œè·³è¿‡ {split} é›†")
        
        if not data_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ° {self.attack_type} æ•°æ®é›†æ–‡ä»¶")
        
        dataset = load_dataset('text', data_files=data_files)
        return dataset


def tokenize_dataset(dataset, tokenizer, max_length=256):
    """Tokenizeæ•°æ®é›†"""
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=max_length,
            padding="max_length",
            return_special_tokens_mask=True,
        )
    
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset["train"].column_names,
        desc="Tokenizing dataset"
    )
    
    return tokenized_dataset


def setup_model_and_tokenizer(model_name: str, gpu_config: GPUConfig, model_config: ModelConfig):
    """
    è®¾ç½®æ¨¡å‹å’Œtokenizer - æ”¯æŒå¤šç§ç°ä»£LLM
    
    Args:
        model_name: æ¨¡å‹åç§°æˆ–HuggingFaceè·¯å¾„
        gpu_config: GPUé…ç½®
        model_config: æ¨¡å‹é…ç½®
    """
    print(f"\nğŸ¤– åŠ è½½æ¨¡å‹: {model_name}")
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=model_config.trust_remote_code,
        padding_side="left",  # Causal LM æ¨è left padding
    )
    
    # è®¾ç½® pad token
    if tokenizer.pad_token is None:
        if tokenizer.eos_token:
            tokenizer.pad_token = tokenizer.eos_token
        else:
            tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    
    # è·å–é‡åŒ–é…ç½®
    quantization_config = get_quantization_config(gpu_config)
    
    # ç¡®å®šç²¾åº¦
    if quantization_config:
        torch_dtype = None  # é‡åŒ–æ—¶è‡ªåŠ¨å¤„ç†
    elif gpu_config.use_bf16:
        torch_dtype = torch.bfloat16
    elif gpu_config.use_fp16:
        torch_dtype = torch.float16
    else:
        torch_dtype = torch.float32
    
    # åŠ è½½æ¨¡å‹ - æ˜¾å¼æŒ‡å®š CUDA è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model_kwargs = {
        "trust_remote_code": model_config.trust_remote_code,
    }
    
    # åªæœ‰åœ¨ä½¿ç”¨é‡åŒ–æ—¶æ‰ç”¨ device_map="auto"
    if gpu_config.load_in_4bit or gpu_config.load_in_8bit:
        model_kwargs["device_map"] = "auto"
    # å¦åˆ™ä¸è®¾ç½® device_mapï¼Œè®© Trainer å¤„ç†è®¾å¤‡åˆ†é…
    
    if quantization_config:
        model_kwargs["quantization_config"] = quantization_config
        print(f"   ä½¿ç”¨é‡åŒ–: {'4-bit' if gpu_config.load_in_4bit else '8-bit'}")
    elif torch_dtype:
        model_kwargs["torch_dtype"] = torch_dtype
        print(f"   ç²¾åº¦: {torch_dtype}")
    
    # Flash Attention 2 æ”¯æŒ - æ£€æŸ¥æ˜¯å¦å¯ç”¨
    use_flash_attn = False
    if gpu_config.use_flash_attention:
        try:
            import flash_attn
            use_flash_attn = True
            model_kwargs["attn_implementation"] = "flash_attention_2"
            print("   Flash Attention 2: enabled")
        except ImportError:
            print("   Flash Attention 2: not installed, using default attention")
    
    # åŠ è½½æ¨¡å‹ï¼Œå¦‚æœ Flash Attention å¤±è´¥åˆ™å›é€€
    try:
        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
    except ImportError as e:
        if "flash_attn" in str(e) or "flash_attention" in str(e).lower():
            print("   Flash Attention 2: failed to load, falling back to default")
            model_kwargs.pop("attn_implementation", None)
            model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
        else:
            raise
    
    # æ˜¾å¼ç§»åŠ¨æ¨¡å‹åˆ° GPUï¼ˆå¦‚æœæ²¡æœ‰ä½¿ç”¨ device_mapï¼‰
    if "device_map" not in model_kwargs and torch.cuda.is_available():
        model = model.to(device)
        print(f"   è®¾å¤‡: {device}")
    
    # è°ƒæ•´ pad token id
    if model.config.pad_token_id is None:
        model.config.pad_token_id = tokenizer.pad_token_id
    
    # å¦‚æœæ·»åŠ äº†æ–°çš„ç‰¹æ®Štokenï¼Œéœ€è¦è°ƒæ•´embedding
    if len(tokenizer) > model.config.vocab_size:
        model.resize_token_embeddings(len(tokenizer))
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"   æ€»å‚æ•°: {total_params / 1e9:.2f}B ({total_params / 1e6:.1f}M)")
    print(f"   å¯è®­ç»ƒ: {trainable_params / 1e9:.2f}B ({trainable_params / 1e6:.1f}M)")
    
    return model, tokenizer


def train(args):
    """è®­ç»ƒå‡½æ•° - æ”¯æŒ Qwen2.5-Coder å’Œå…¶ä»–ç°ä»£ LLM"""
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # æ ¹æ®é¢„è®¾æˆ–è‡ªå®šä¹‰å‚æ•°åŠ è½½é…ç½®
    if args.model_preset and args.model_preset in MODEL_PRESETS:
        preset = MODEL_PRESETS[args.model_preset]
        model_name = preset["model_name"]
        max_length = args.max_length or preset.get("max_length", 256)
        batch_size = args.batch_size or preset.get("batch_size", 4)
        gradient_accumulation = args.gradient_accumulation or preset.get("gradient_accumulation", 8)
        use_flash_attention = preset.get("use_flash_attention", True)
        load_in_4bit = preset.get("load_in_4bit", False)
        gradient_checkpointing = preset.get("gradient_checkpointing", False) or args.gradient_checkpointing
    else:
        model_name = args.model_name
        max_length = args.max_length or 256
        batch_size = args.batch_size or 4
        gradient_accumulation = args.gradient_accumulation or 8
        use_flash_attention = args.flash_attention
        load_in_4bit = args.load_in_4bit
        gradient_checkpointing = args.gradient_checkpointing
    
    # åˆ›å»ºé…ç½®
    model_config = ModelConfig(
        model_name=model_name,
        max_length=max_length,
        trust_remote_code=True,
    )
    
    gpu_config = GPUConfig(
        use_fp16=not args.bf16,
        use_bf16=args.bf16,
        use_flash_attention=use_flash_attention,
        load_in_4bit=load_in_4bit,
        gradient_checkpointing=gradient_checkpointing,
    )
    
    train_config = TrainingConfig(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation,
        learning_rate=args.learning_rate,
        bf16=args.bf16,
        fp16=not args.bf16,
        optim=args.optim,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        eval_steps=args.eval_steps,
    )
    
    print("="*60)
    print(f"ğŸš€ é¢„è®­ç»ƒé…ç½® - {args.attack_type.upper()}")
    print("="*60)
    print(f"æ¨¡å‹: {model_name}")
    print(f"é¢„è®¾: {args.model_preset or 'è‡ªå®šä¹‰'}")
    print(f"æ”»å‡»ç±»å‹: {args.attack_type}")
    print(f"æœ€å¤§é•¿åº¦: {max_length}")
    print(f"Batch size: {batch_size}")
    print(f"Accumulation: {gradient_accumulation}")
    print(f"ç­‰æ•ˆbatch: {batch_size * gradient_accumulation}")
    print(f"å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"Epochs: {args.epochs}")
    print(f"ç²¾åº¦: {'BF16' if args.bf16 else 'FP16'}")
    print(f"4-bité‡åŒ–: {load_in_4bit}")
    print(f"Flash Attention: {use_flash_attention}")
    print(f"æ¢¯åº¦æ£€æŸ¥ç‚¹: {gradient_checkpointing}")
    print(f"ä¼˜åŒ–å™¨: {args.optim}")
    print("="*60)
    
    # æ£€æŸ¥GPU
    if torch.cuda.is_available():
        print(f"\nğŸ–¥ï¸  GPU: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
    else:
        print("\nâš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
    
    # åŠ è½½æ•°æ®é›†
    print(f"\nğŸ“‚ åŠ è½½ {args.attack_type} æ•°æ®é›†...")
    dataset_loader = PayloadDataset(args.data_dir, args.attack_type)
    dataset = dataset_loader.load()
    
    print(f"   è®­ç»ƒé›†: {len(dataset['train'])} æ¡")
    if 'validation' in dataset:
        print(f"   éªŒè¯é›†: {len(dataset['validation'])} æ¡")
    
    # è®¾ç½®æ¨¡å‹å’Œtokenizer
    model, tokenizer = setup_model_and_tokenizer(model_name, gpu_config, model_config)
    
    # éªŒè¯æ¨¡å‹åœ¨ GPU ä¸Š
    if torch.cuda.is_available():
        # æ£€æŸ¥æ¨¡å‹å‚æ•°æ‰€åœ¨è®¾å¤‡
        param_device = next(model.parameters()).device
        print(f"   æ¨¡å‹è®¾å¤‡: {param_device}")
        if param_device.type != "cuda":
            print("   âš ï¸ æ¨¡å‹ä¸åœ¨ GPU ä¸Šï¼Œæ­£åœ¨ç§»åŠ¨...")
            model = model.cuda()
            print(f"   æ¨¡å‹å·²ç§»åŠ¨åˆ°: {next(model.parameters()).device}")
        
        # æ˜¾ç¤ºå½“å‰ GPU å†…å­˜ä½¿ç”¨
        print(f"   GPU å†…å­˜ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    
    # å¯ç”¨ gradient checkpointing (èŠ‚çœæ˜¾å­˜)
    if gpu_config.gradient_checkpointing:
        model.gradient_checkpointing_enable()
        print("âœ… å¯ç”¨ gradient checkpointing")
    
    # Tokenize æ•°æ®é›†
    print("\nğŸ“ Tokenizing æ•°æ®é›†...")
    tokenized_dataset = tokenize_dataset(
        dataset, 
        tokenizer, 
        max_length=max_length
    )
    
    # æ•°æ® collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # Causal LM
    )
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=train_config.output_dir,
        
        num_train_epochs=train_config.num_train_epochs,
        per_device_train_batch_size=train_config.per_device_train_batch_size,
        per_device_eval_batch_size=train_config.per_device_eval_batch_size,
        gradient_accumulation_steps=train_config.gradient_accumulation_steps,
        
        learning_rate=train_config.learning_rate,
        weight_decay=train_config.weight_decay,
        adam_beta1=train_config.adam_beta1,
        adam_beta2=train_config.adam_beta2,
        adam_epsilon=train_config.adam_epsilon,
        max_grad_norm=train_config.max_grad_norm,
        
        lr_scheduler_type=train_config.lr_scheduler_type,
        warmup_ratio=train_config.warmup_ratio,
        
        logging_dir=f"{train_config.output_dir}/logs",
        logging_steps=train_config.logging_steps,
        
        save_strategy="steps",
        save_steps=train_config.save_steps,
        save_total_limit=train_config.save_total_limit,
        
        eval_strategy=train_config.evaluation_strategy if 'validation' in dataset else "no",
        eval_steps=train_config.eval_steps if 'validation' in dataset else None,
        
        load_best_model_at_end=train_config.load_best_model_at_end if 'validation' in dataset else False,
        metric_for_best_model=train_config.metric_for_best_model if 'validation' in dataset else None,
        
        bf16=train_config.bf16,
        fp16=train_config.fp16,
        
        optim=train_config.optim,
        
        dataloader_num_workers=train_config.dataloader_num_workers,
        dataloader_pin_memory=train_config.dataloader_pin_memory,
        
        seed=train_config.seed,
        report_to=["tensorboard"],
        
        gradient_checkpointing=gpu_config.gradient_checkpointing,
    )
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset["train"],
        eval_dataset=tokenized_dataset.get("validation"),
        data_collator=data_collator,
        processing_class=tokenizer,
    )
    
    # æ£€æµ‹ checkpoint å¹¶æ¢å¤
    resume_from_checkpoint = None
    if args.resume:
        checkpoint_dir = Path(train_config.output_dir)
        checkpoints = list(checkpoint_dir.glob("checkpoint-*"))
        if checkpoints:
            # æ‰¾åˆ°æœ€æ–°çš„ checkpointï¼ˆæŒ‰æ­¥æ•°æ’åºï¼‰
            def get_step(ckpt):
                try:
                    return int(ckpt.name.split("-")[1])
                except (IndexError, ValueError):
                    return 0
            latest_checkpoint = max(checkpoints, key=get_step)
            resume_from_checkpoint = str(latest_checkpoint)
            print(f"\nğŸ”„ æ£€æµ‹åˆ° checkpoint: {latest_checkpoint.name}")
            print(f"   å°†ä» step {get_step(latest_checkpoint)} æ¢å¤è®­ç»ƒ")
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "="*60)
    if resume_from_checkpoint:
        print(f"ğŸƒ ä» checkpoint æ¢å¤è®­ç»ƒ...")
    else:
        print("ğŸƒ å¼€å§‹è®­ç»ƒ...")
    print("="*60)
    
    train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)
    
    # ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    trainer.save_model()
    trainer.save_state()
    
    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    
    # è¯„ä¼°
    if 'validation' in dataset:
        print("\nğŸ“Š è¯„ä¼°æ¨¡å‹...")
        eval_metrics = trainer.evaluate()
        trainer.log_metrics("eval", eval_metrics)
        trainer.save_metrics("eval", eval_metrics)
        
        import math
        perplexity = math.exp(eval_metrics['eval_loss']) if eval_metrics['eval_loss'] < 20 else float('inf')
        print(f"\n   éªŒè¯ Loss: {eval_metrics['eval_loss']:.4f}")
        print(f"   éªŒè¯ Perplexity: {perplexity:.2f}")
    
    print("\n" + "="*60)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print(f"   æ¨¡å‹ä¿å­˜åœ¨: {train_config.output_dir}")
    print("="*60)


def main():
    parser = argparse.ArgumentParser(
        description="é¢„è®­ç»ƒè„šæœ¬ - æ”¯æŒ Qwen2.5-Coder å’Œå…¶ä»–ç°ä»£ LLM",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨ Qwen2.5-Coder-1.5B é¢„è®¾ (æ¨è)
  python pretrain.py --model-preset qwen2.5-coder-1.5b --attack-type sqli

  # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹
  python pretrain.py --model-name Qwen/Qwen2.5-Coder-0.5B --attack-type xss

  # ä½¿ç”¨ 4-bit é‡åŒ– (å¤§æ¨¡å‹)
  python pretrain.py --model-preset qwen2.5-coder-3b --attack-type sqli

å¯ç”¨é¢„è®¾:
  - qwen2.5-coder-0.5b  (æ¨èå¿«é€Ÿå®éªŒ)
  - qwen2.5-coder-1.5b  (æ¨èï¼Œå¹³è¡¡æ€§èƒ½å’Œæ˜¾å­˜)
  - qwen2.5-coder-3b    (éœ€è¦4-bité‡åŒ–)
  - deepseek-coder-1.3b (ä»£ç ä¸“ç”¨)
  - phi-3-mini          (éœ€è¦4-bité‡åŒ–)
  - gpt2                (å…¼å®¹æ—§ç‰ˆ)
        """
    )
    
    # æ¨¡å‹é€‰æ‹©
    model_group = parser.add_argument_group("æ¨¡å‹é…ç½®")
    model_group.add_argument("--model-preset", type=str, default=DEFAULT_MODEL,
                            choices=list(MODEL_PRESETS.keys()),
                            help=f"æ¨¡å‹é¢„è®¾ (é»˜è®¤: {DEFAULT_MODEL})")
    model_group.add_argument("--model-name", type=str, default=None,
                            help="è‡ªå®šä¹‰æ¨¡å‹åç§° (è¦†ç›–é¢„è®¾)")
    model_group.add_argument("--max-length", type=int, default=None,
                            help="æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: æ ¹æ®é¢„è®¾)")
    
    # æ•°æ®å‚æ•°
    data_group = parser.add_argument_group("æ•°æ®é…ç½®")
    data_group.add_argument("--data-dir", type=str, default="data/processed",
                           help="å¤„ç†åçš„æ•°æ®ç›®å½•")
    data_group.add_argument("--attack-type", type=str, default="sqli",
                           choices=["sqli", "xss", "rce"],
                           help="æ”»å‡»ç±»å‹ (é»˜è®¤: sqli)")
    
    # è®­ç»ƒå‚æ•°
    train_group = parser.add_argument_group("è®­ç»ƒé…ç½®")
    train_group.add_argument("--output-dir", type=str, default=None,
                            help="è¾“å‡ºç›®å½• (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆ)")
    train_group.add_argument("--epochs", type=int, default=3,
                            help="è®­ç»ƒè½®æ•° (é»˜è®¤: 3)")
    train_group.add_argument("--batch-size", type=int, default=None,
                            help="batch size (é»˜è®¤: æ ¹æ®é¢„è®¾)")
    train_group.add_argument("--gradient-accumulation", type=int, default=None,
                            help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•° (é»˜è®¤: æ ¹æ®é¢„è®¾)")
    train_group.add_argument("--learning-rate", type=float, default=2e-5,
                            help="å­¦ä¹ ç‡ (é»˜è®¤: 2e-5)")
    
    # ä¼˜åŒ–å‚æ•°
    optim_group = parser.add_argument_group("ä¼˜åŒ–é…ç½®")
    optim_group.add_argument("--bf16", action="store_true", default=True,
                            help="ä½¿ç”¨ BF16 ç²¾åº¦ (æ¨èï¼Œé»˜è®¤å¼€å¯)")
    optim_group.add_argument("--no-bf16", action="store_true",
                            help="ç¦ç”¨ BF16ï¼Œä½¿ç”¨ FP16")
    optim_group.add_argument("--load-in-4bit", action="store_true",
                            help="ä½¿ç”¨ 4-bit é‡åŒ–")
    optim_group.add_argument("--flash-attention", action="store_true", default=True,
                            help="ä½¿ç”¨ Flash Attention 2")
    optim_group.add_argument("--gradient-checkpointing", action="store_true",
                            help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ (èŠ‚çœæ˜¾å­˜)")
    optim_group.add_argument("--optim", type=str, default="adamw_torch",
                            help="ä¼˜åŒ–å™¨ç±»å‹ (é»˜è®¤: adamw_torch)")
    
    # æ—¥å¿—å‚æ•°
    log_group = parser.add_argument_group("æ—¥å¿—é…ç½®")
    log_group.add_argument("--logging-steps", type=int, default=50,
                          help="æ—¥å¿—è®°å½•æ­¥æ•°")
    log_group.add_argument("--save-steps", type=int, default=500,
                          help="æ¨¡å‹ä¿å­˜æ­¥æ•°")
    log_group.add_argument("--eval-steps", type=int, default=500,
                          help="è¯„ä¼°æ­¥æ•°")
    
    # æ–­ç‚¹ç»­è®­
    resume_group = parser.add_argument_group("æ–­ç‚¹ç»­è®­")
    resume_group.add_argument("--resume", action="store_true", default=True,
                             help="è‡ªåŠ¨ä»æœ€æ–° checkpoint æ¢å¤è®­ç»ƒ (é»˜è®¤å¼€å¯)")
    resume_group.add_argument("--no-resume", action="store_true",
                             help="ç¦ç”¨è‡ªåŠ¨æ¢å¤ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # å…¶ä»–
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    # å¤„ç† resume å‚æ•°
    if args.no_resume:
        args.resume = False
    
    # å¤„ç† bf16 å‚æ•°
    if args.no_bf16:
        args.bf16 = False
    
    # è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•
    if args.output_dir is None:
        preset_name = args.model_preset.replace(".", "_").replace("-", "_")
        args.output_dir = f"models/pretrain_{args.attack_type}_{preset_name}"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)
    
    # å¼€å§‹è®­ç»ƒ
    train(args)


if __name__ == "__main__":
    main()
