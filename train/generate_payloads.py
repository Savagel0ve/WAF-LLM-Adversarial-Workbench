"""
ä½¿ç”¨è®­ç»ƒå¥½çš„RLæ¨¡å‹æ‰¹é‡ç”ŸæˆWAFç»•è¿‡è½½è·
æ”¯æŒå¤šç§ç”Ÿæˆç­–ç•¥å’Œè¾“å‡ºæ ¼å¼
"""

import os
import sys
import json
import torch
import argparse
from typing import List, Dict, Set
from datetime import datetime
from tqdm import tqdm
from transformers import GPT2Tokenizer, AutoModelForCausalLM


class PayloadGenerator:
    """è½½è·ç”Ÿæˆå™¨"""
    
    def __init__(self, model_path: str, device: str = "cuda"):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            model_path: RLæ¨¡å‹è·¯å¾„
            device: è¿è¡Œè®¾å¤‡
        """
        self.device = device if torch.cuda.is_available() else "cpu"
        
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½æ¨¡å‹
        self.tokenizer = GPT2Tokenizer.from_pretrained(model_path, padding_side="left")
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model = AutoModelForCausalLM.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ“ æ¨¡å‹å·²åŠ è½½ (è®¾å¤‡: {self.device})")
    
    def generate_batch(
        self,
        batch_size: int = 10,
        max_length: int = 128,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.95,
        do_sample: bool = True,
        num_beams: int = 1,
    ) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆè½½è·
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            max_length: æœ€å¤§é•¿åº¦
            temperature: æ¸©åº¦å‚æ•°
            top_k: Top-Ké‡‡æ ·
            top_p: Nucleusé‡‡æ ·
            do_sample: æ˜¯å¦é‡‡æ ·
            num_beams: Beam searchæŸå®½
            
        Returns:
            ç”Ÿæˆçš„è½½è·åˆ—è¡¨
        """
        payloads = []
        
        with torch.no_grad():
            # åˆ›å»ºæ‰¹æ¬¡è¾“å…¥
            input_ids = torch.tensor(
                [[self.tokenizer.bos_token_id]] * batch_size,
                dtype=torch.long,
                device=self.device
            )
            
            attention_mask = torch.ones_like(input_ids)

            # ç”Ÿæˆ
            outputs = self.model.generate(
                input_ids,
                attention_mask=attention_mask,
                max_length=max_length,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                do_sample=do_sample,
                num_beams=num_beams,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                num_return_sequences=1,
            )
            
            # è§£ç 
            for output in outputs:
                payload = self.tokenizer.decode(output, skip_special_tokens=True)
                payloads.append(payload)
        
        return payloads
    
    def generate_diverse(
        self,
        num_samples: int = 100,
        batch_size: int = 10,
        max_length: int = 128,
        temperature_range: tuple = (0.8, 1.2),
        show_progress: bool = True,
    ) -> List[str]:
        """
        ç”Ÿæˆå¤šæ ·åŒ–çš„è½½è·ï¼ˆä½¿ç”¨ä¸åŒæ¸©åº¦ï¼‰
        
        Args:
            num_samples: æ€»æ ·æœ¬æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            max_length: æœ€å¤§é•¿åº¦
            temperature_range: æ¸©åº¦èŒƒå›´
            show_progress: æ˜¾ç¤ºè¿›åº¦æ¡
            
        Returns:
            ç”Ÿæˆçš„è½½è·åˆ—è¡¨
        """
        payloads = []
        num_batches = (num_samples + batch_size - 1) // batch_size
        
        iterator = tqdm(range(num_batches), desc="ç”Ÿæˆè½½è·") if show_progress else range(num_batches)
        
        for i in iterator:
            # åŠ¨æ€æ¸©åº¦
            temp = temperature_range[0] + (temperature_range[1] - temperature_range[0]) * (i / num_batches)
            
            # å½“å‰æ‰¹æ¬¡å¤§å°
            current_batch_size = min(batch_size, num_samples - len(payloads))
            
            # ç”Ÿæˆ
            batch = self.generate_batch(
                batch_size=current_batch_size,
                max_length=max_length,
                temperature=temp,
            )
            
            payloads.extend(batch)
        
        return payloads[:num_samples]
    
    def deduplicate(self, payloads: List[str]) -> List[str]:
        """å»é‡"""
        return list(dict.fromkeys(payloads))  # ä¿æŒé¡ºåºçš„å»é‡
    
    def filter_valid(self, payloads: List[str], min_length: int = 5) -> List[str]:
        """è¿‡æ»¤æ— æ•ˆè½½è·"""
        return [p for p in payloads if len(p.strip()) >= min_length]
    
    def save_to_file(
        self,
        payloads: List[str],
        output_file: str,
        format: str = "txt"
    ):
        """
        ä¿å­˜åˆ°æ–‡ä»¶
        
        Args:
            payloads: è½½è·åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶
            format: æ ¼å¼ (txt/json/csv)
        """
        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)
        
        if format == "txt":
            with open(output_file, 'w', encoding='utf-8') as f:
                for payload in payloads:
                    f.write(f"{payload}\n")
        
        elif format == "json":
            data = {
                'generated_at': datetime.now().isoformat(),
                'total': len(payloads),
                'payloads': payloads
            }
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        
        elif format == "csv":
            import csv
            with open(output_file, 'w', encoding='utf-8', newline='') as f:
                writer = csv.writer(f)
                writer.writerow(['payload'])
                for payload in payloads:
                    writer.writerow([payload])
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
        
        print(f"âœ“ å·²ä¿å­˜ {len(payloads)} ä¸ªè½½è·åˆ°: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡ç”ŸæˆWAFç»•è¿‡è½½è·")
    
    # å¿…éœ€å‚æ•°
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="RLæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--output_file",
        type=str,
        required=True,
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„"
    )
    
    # ç”Ÿæˆå‚æ•°
    parser.add_argument("--num_samples", type=int, default=1000, help="ç”Ÿæˆæ•°é‡")
    parser.add_argument("--batch_size", type=int, default=10, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_length", type=int, default=128, help="æœ€å¤§é•¿åº¦")
    parser.add_argument("--temperature", type=float, default=1.0, help="æ¸©åº¦ (å›ºå®šå€¼)")
    parser.add_argument("--use_diverse", action="store_true", help="ä½¿ç”¨å¤šæ ·åŒ–ç”Ÿæˆï¼ˆåŠ¨æ€æ¸©åº¦ï¼‰")
    parser.add_argument("--temp_min", type=float, default=0.8, help="æœ€å°æ¸©åº¦")
    parser.add_argument("--temp_max", type=float, default=1.2, help="æœ€å¤§æ¸©åº¦")
    
    # åå¤„ç†
    parser.add_argument("--deduplicate", action="store_true", help="å»é‡")
    parser.add_argument("--min_length", type=int, default=5, help="æœ€å°é•¿åº¦")
    
    # è¾“å‡ºæ ¼å¼
    parser.add_argument("--format", type=str, default="txt", choices=["txt", "json", "csv"], help="è¾“å‡ºæ ¼å¼")
    
    # å…¶ä»–
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡")
    parser.add_argument("--seed", type=int, default=None, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        torch.manual_seed(args.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(args.seed)
    
    # æ£€æŸ¥æ¨¡å‹
    if not os.path.exists(args.model_path):
        print(f"âŒ é”™è¯¯: æ¨¡å‹ä¸å­˜åœ¨: {args.model_path}")
        sys.exit(1)
    
    print("="*80)
    print("ğŸš€ è½½è·æ‰¹é‡ç”Ÿæˆå·¥å…·")
    print("="*80)
    print(f"æ¨¡å‹: {args.model_path}")
    print(f"ç”Ÿæˆæ•°é‡: {args.num_samples}")
    print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"è¾“å‡ºæ–‡ä»¶: {args.output_file}")
    print(f"æ ¼å¼: {args.format}")
    print("="*80 + "\n")
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = PayloadGenerator(args.model_path, args.device)
    
    # ç”Ÿæˆè½½è·
    print(f"\nğŸ“ å¼€å§‹ç”Ÿæˆ {args.num_samples} ä¸ªè½½è·...")
    start_time = datetime.now()
    
    if args.use_diverse:
        print(f"   ä½¿ç”¨å¤šæ ·åŒ–ç”Ÿæˆ (æ¸©åº¦: {args.temp_min} ~ {args.temp_max})")
        payloads = generator.generate_diverse(
            num_samples=args.num_samples,
            batch_size=args.batch_size,
            max_length=args.max_length,
            temperature_range=(args.temp_min, args.temp_max),
            show_progress=True,
        )
    else:
        print(f"   ä½¿ç”¨å›ºå®šæ¸©åº¦: {args.temperature}")
        payloads = []
        num_batches = (args.num_samples + args.batch_size - 1) // args.batch_size
        
        for i in tqdm(range(num_batches), desc="ç”Ÿæˆè½½è·"):
            current_batch_size = min(args.batch_size, args.num_samples - len(payloads))
            batch = generator.generate_batch(
                batch_size=current_batch_size,
                max_length=args.max_length,
                temperature=args.temperature,
            )
            payloads.extend(batch)
        
        payloads = payloads[:args.num_samples]
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    print(f"\nâœ“ ç”Ÿæˆå®Œæˆ! è€—æ—¶: {duration:.2f}ç§’")
    print(f"  ç”Ÿæˆé€Ÿåº¦: {len(payloads)/duration:.2f} ä¸ª/ç§’")
    
    # åå¤„ç†
    original_count = len(payloads)
    
    # å»é‡
    if args.deduplicate:
        print(f"\nğŸ”„ å»é‡...")
        payloads = generator.deduplicate(payloads)
        print(f"  å»é‡å‰: {original_count}")
        print(f"  å»é‡å: {len(payloads)}")
        print(f"  é‡å¤ç‡: {(1 - len(payloads)/original_count)*100:.1f}%")
    
    # è¿‡æ»¤
    payloads = generator.filter_valid(payloads, args.min_length)
    print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    print(f"  æœ‰æ•ˆè½½è·: {len(payloads)}")
    print(f"  å¹³å‡é•¿åº¦: {sum(len(p) for p in payloads)/len(payloads):.1f} å­—ç¬¦")
    
    # ä¿å­˜
    print(f"\nğŸ’¾ ä¿å­˜åˆ°æ–‡ä»¶...")
    generator.save_to_file(payloads, args.output_file, args.format)
    
    # æ˜¾ç¤ºæ ·ä¾‹
    print(f"\nğŸ“„ æ ·ä¾‹ (å‰5ä¸ª):")
    print("="*80)
    for i, payload in enumerate(payloads[:5]):
        print(f"\n[{i+1}] {payload}")
    print("="*80)
    
    print(f"\nâœ… å®Œæˆ!")
    print(f"   è¾“å‡ºæ–‡ä»¶: {args.output_file}")
    print(f"   è½½è·æ•°é‡: {len(payloads)}")


if __name__ == "__main__":
    main()
