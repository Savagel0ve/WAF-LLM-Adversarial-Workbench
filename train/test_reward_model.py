"""
æµ‹è¯•å¥–åŠ±æ¨¡å‹ - æ¨ç†å’Œè¯„ä¼°
æ”¯æŒ Qwen2.5-Coder å’Œå…¶ä»–ç°ä»£ LLM

ä½¿ç”¨è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹é¢„æµ‹payloadçš„WAFç»•è¿‡æ¦‚ç‡
"""
import torch
import argparse
from pathlib import Path
from typing import List, Dict
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import logging


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RewardModelInference:
    """å¥–åŠ±æ¨¡å‹æ¨ç†å™¨ - æ”¯æŒ Qwen2.5-Coder å’Œå…¶ä»–ç°ä»£ LLM"""
    
    def __init__(self, model_path: str, device: str = None):
        """
        åˆå§‹åŒ–æ¨ç†å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ (cuda/cpu)
        """
        self.model_path = Path(model_path)
        
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        logger.info(f"ğŸ” åŠ è½½æ¨¡å‹: {self.model_path}")
        logger.info(f"   è®¾å¤‡: {self.device}")
        
        # åŠ è½½ tokenizer å’Œæ¨¡å‹ (ä½¿ç”¨ Auto ç±»æ”¯æŒå„ç§æ¨¡å‹)
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True,
        )
        
        # è®¾ç½® pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
        )
        self.model.to(self.device)
        self.model.eval()
        
        logger.info(f"   æ¨¡å‹ç±»å‹: {type(self.model).__name__}")
        logger.info("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def predict_single(self, payload: str, return_logits: bool = False) -> float:
        """
        é¢„æµ‹å•ä¸ªpayloadçš„ç»•è¿‡æ¦‚ç‡
        
        Args:
            payload: æ”»å‡»payload
            return_logits: æ˜¯å¦è¿”å›åŸå§‹logits
            
        Returns:
            ç»•è¿‡æ¦‚ç‡ [0, 1]
        """
        # Tokenize
        inputs = self.tokenizer(
            payload,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=128
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # æ¨ç†
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits.squeeze()
        
        if return_logits:
            return logits.item()
        
        # è½¬æ¢ä¸ºæ¦‚ç‡
        prob = torch.sigmoid(logits).item()
        return prob
    
    def predict_batch(self, payloads: List[str], batch_size: int = 32) -> List[float]:
        """
        æ‰¹é‡é¢„æµ‹
        
        Args:
            payloads: payloadåˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            æ¦‚ç‡åˆ—è¡¨
        """
        probs = []
        
        for i in range(0, len(payloads), batch_size):
            batch = payloads[i:i + batch_size]
            
            # Tokenize
            inputs = self.tokenizer(
                batch,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=128
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # æ¨ç†
            with torch.no_grad():
                outputs = self.model(**inputs)
                logits = outputs.logits.squeeze()
            
            # è½¬æ¢ä¸ºæ¦‚ç‡
            batch_probs = torch.sigmoid(logits).cpu().numpy()
            
            if len(batch) == 1:
                batch_probs = [batch_probs.item()]
            else:
                batch_probs = batch_probs.tolist()
            
            probs.extend(batch_probs)
        
        return probs
    
    def evaluate_payloads(self, payloads: List[str]) -> Dict:
        """
        è¯„ä¼°ä¸€ç»„payload
        
        Args:
            payloads: payloadåˆ—è¡¨
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        logger.info(f"è¯„ä¼° {len(payloads)} ä¸ªpayload...")
        
        probs = self.predict_batch(payloads)
        
        # ç»Ÿè®¡
        import numpy as np
        probs_array = np.array(probs)
        
        stats = {
            "count": len(probs),
            "mean": float(np.mean(probs_array)),
            "std": float(np.std(probs_array)),
            "min": float(np.min(probs_array)),
            "max": float(np.max(probs_array)),
            "median": float(np.median(probs_array)),
            "high_confidence_bypass": int(np.sum(probs_array > 0.8)),
            "low_confidence_bypass": int(np.sum(probs_array < 0.2)),
        }
        
        return stats


def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•å¥–åŠ±æ¨¡å‹")
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--payload",
        type=str,
        help="å•ä¸ªpayload (ç”¨äºå¿«é€Ÿæµ‹è¯•)"
    )
    parser.add_argument(
        "--payload_file",
        type=str,
        help="payloadæ–‡ä»¶ (æ¯è¡Œä¸€ä¸ª)"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=32,
        help="æ‰¹æ¬¡å¤§å°"
    )
    
    args = parser.parse_args()
    
    logger.info("="*60)
    logger.info("å¥–åŠ±æ¨¡å‹æ¨ç†æµ‹è¯•")
    logger.info("="*60)
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    inferencer = RewardModelInference(args.model_path)
    
    # å•ä¸ªpayloadæµ‹è¯•
    if args.payload:
        logger.info(f"\næµ‹è¯•payload: {args.payload}")
        prob = inferencer.predict_single(args.payload)
        logger.info(f"ç»•è¿‡æ¦‚ç‡: {prob:.4f}")
        
        if prob > 0.8:
            logger.info("ğŸŸ¢ é«˜æ¦‚ç‡ç»•è¿‡")
        elif prob > 0.5:
            logger.info("ğŸŸ¡ ä¸­ç­‰æ¦‚ç‡ç»•è¿‡")
        else:
            logger.info("ğŸ”´ ä½æ¦‚ç‡ç»•è¿‡")
    
    # æ–‡ä»¶æµ‹è¯•
    elif args.payload_file:
        logger.info(f"\nä»æ–‡ä»¶åŠ è½½payload: {args.payload_file}")
        
        with open(args.payload_file, 'r', encoding='utf-8') as f:
            payloads = [line.strip() for line in f if line.strip()]
        
        logger.info(f"åŠ è½½ {len(payloads)} ä¸ªpayload")
        
        # è¯„ä¼°
        stats = inferencer.evaluate_payloads(payloads)
        
        logger.info("\n" + "="*60)
        logger.info("è¯„ä¼°ç»“æœ:")
        for key, value in stats.items():
            if isinstance(value, float):
                logger.info(f"  {key}: {value:.4f}")
            else:
                logger.info(f"  {key}: {value}")
        
        # æ˜¾ç¤ºç¤ºä¾‹
        logger.info("\n" + "="*60)
        logger.info("ç¤ºä¾‹é¢„æµ‹ (å‰10ä¸ª):")
        
        probs = inferencer.predict_batch(payloads[:10])
        for payload, prob in zip(payloads[:10], probs):
            status = "ğŸŸ¢" if prob > 0.8 else "ğŸŸ¡" if prob > 0.5 else "ğŸ”´"
            logger.info(f"{status} {prob:.4f} | {payload[:60]}")
    
    else:
        # äº¤äº’å¼æµ‹è¯•
        logger.info("\nè¿›å…¥äº¤äº’å¼æµ‹è¯•æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
        
        while True:
            try:
                payload = input("\nè¯·è¾“å…¥payload: ").strip()
                
                if payload.lower() in ['quit', 'exit', 'q']:
                    break
                
                if not payload:
                    continue
                
                prob = inferencer.predict_single(payload)
                
                if prob > 0.8:
                    status = "ğŸŸ¢ é«˜æ¦‚ç‡ç»•è¿‡"
                elif prob > 0.5:
                    status = "ğŸŸ¡ ä¸­ç­‰æ¦‚ç‡ç»•è¿‡"
                else:
                    status = "ğŸ”´ ä½æ¦‚ç‡ç»•è¿‡"
                
                logger.info(f"ç»•è¿‡æ¦‚ç‡: {prob:.4f} | {status}")
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                logger.error(f"é”™è¯¯: {e}")
    
    logger.info("\nâœ… å®Œæˆ!")


if __name__ == "__main__":
    main()
