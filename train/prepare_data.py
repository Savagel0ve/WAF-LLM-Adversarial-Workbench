"""
æ•°æ®å‡†å¤‡å’Œé¢„å¤„ç†è„šæœ¬
è§£å‹æ•°æ®é›†ã€æ¸…æ´—ã€åˆ†å‰²ã€tokenize

ä¼˜åŒ–è¯´æ˜:
- ä½¿ç”¨è“„æ°´æ± é‡‡æ ·ç®—æ³•é¿å…åŠ è½½å…¨éƒ¨æ•°æ®åˆ°å†…å­˜
- æŒ‰ç…§GPTFuzzerè®ºæ–‡è¦æ±‚é™åˆ¶æ•°æ®é‡: SQLi/XSS=512K, RCE=37K
"""
import os
import zipfile
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
import json
from tqdm import tqdm
import random


class DatasetPreparer:
    """æ•°æ®é›†å‡†å¤‡å™¨"""
    
    def __init__(self, data_dir="gptfuzzer-main/Datasets", output_dir="data/processed"):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.attack_types = {
            "sqli": "SQLi",
            "xss": "XSS",
            "rce": "RCE"
        }
        
        # æ ¹æ®GPTFuzzerè®ºæ–‡çš„æ•°æ®é‡è¦æ±‚
        self.max_samples = {
            "sqli": 512000,  # 512K
            "xss": 512000,   # 512K
            "rce": None      # RCEä½¿ç”¨å…¨é‡æ•°æ® (37,302)
        }
    
    def extract_sqli_dataset(self):
        """è§£å‹SQLiæ•°æ®é›†(åˆ†å·å‹ç¼©)"""
        print("\n" + "="*60)
        print("è§£å‹SQLiæ•°æ®é›†")
        print("="*60)
        
        sqli_dir = self.data_dir / "SQLi"
        zip_file = sqli_dir / "SQLi_Dataset.zip"
        
        if not zip_file.exists():
            print(f"âš ï¸  è­¦å‘Š: {zip_file} ä¸å­˜åœ¨")
            print("è¯·ç¡®ä¿SQLi_Dataset.zipåŠå…¶åˆ†å·æ–‡ä»¶(.z01, .z02, ...)éƒ½åœ¨ç›®å½•ä¸­")
            
            # æ£€æŸ¥åˆ†å·æ–‡ä»¶
            z_files = list(sqli_dir.glob("SQLi_Dataset.z*"))
            if z_files:
                print(f"âœ… æ‰¾åˆ° {len(z_files)} ä¸ªåˆ†å·æ–‡ä»¶")
                print("   éœ€è¦å…ˆåˆå¹¶åˆ†å·æ–‡ä»¶:")
                print("   æ–¹æ³•1: ä½¿ç”¨7-Zipè§£å‹ SQLi_Dataset.zip")
                print("   æ–¹æ³•2: åœ¨Windowsä¸­ï¼Œå³é”®SQLi_Dataset.zipé€‰æ‹©'è§£å‹åˆ°...'")
            return False
        
        # è§£å‹
        try:
            print(f"ğŸ“¦ æ­£åœ¨è§£å‹ {zip_file}...")
            with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                zip_ref.extractall(sqli_dir)
            print("âœ… SQLiæ•°æ®é›†è§£å‹æˆåŠŸ")
            return True
        except zipfile.BadZipFile:
            print("âŒ è§£å‹å¤±è´¥: å¯èƒ½éœ€è¦å…ˆåˆå¹¶åˆ†å·æ–‡ä»¶")
            print("   è¯·ä½¿ç”¨7-Zipæˆ–WinRARè§£å‹å®Œæ•´çš„zipæ–‡ä»¶")
            return False
    
    def load_dataset(self, attack_type: str) -> List[str]:
        """
        åŠ è½½æŒ‡å®šæ”»å‡»ç±»å‹çš„æ•°æ®é›†
        å¦‚æœæ•°æ®é‡è¶…è¿‡è®ºæ–‡è¦æ±‚ï¼Œä½¿ç”¨å¿«é€Ÿéšæœºé‡‡æ ·
        """
        attack_dir = self.attack_types.get(attack_type.lower())
        if not attack_dir:
            raise ValueError(f"æœªçŸ¥çš„æ”»å‡»ç±»å‹: {attack_type}")
        
        dataset_file = self.data_dir / attack_dir / f"{attack_dir}_Dataset.txt"
        
        if not dataset_file.exists():
            print(f"âš ï¸  è­¦å‘Š: {dataset_file} ä¸å­˜åœ¨")
            return []
        
        print(f"\nğŸ“‚ åŠ è½½ {attack_type.upper()} æ•°æ®é›†: {dataset_file}")
        
        # è·å–è¯¥æ”»å‡»ç±»å‹çš„æœ€å¤§æ ·æœ¬æ•°
        max_size = self.max_samples.get(attack_type.lower())
        
        # å¦‚æœæ²¡æœ‰é™åˆ¶ï¼Œæ­£å¸¸åŠ è½½
        if max_size is None:
            payloads = []
            with open(dataset_file, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        payloads.append(line)
            print(f"âœ… åŠ è½½ {len(payloads):,} æ¡ {attack_type.upper()} payloads (å…¨é‡)")
            return payloads
        
        # å¿«é€Ÿé‡‡æ ·ç­–ç•¥ï¼šè¯»å–å›ºå®šæ•°é‡ååœæ­¢
        # è¿‡é‡‡æ ·2å€ä»¥åº”å¯¹åç»­æ¸…æ´—
        sample_size = max_size * 2
        print(f"  è®ºæ–‡è¦æ±‚: {max_size:,} æ¡")
        print(f"  å¿«é€Ÿé‡‡æ ·ç­–ç•¥: è¯»å–å‰ {sample_size:,} æ¡æœ‰æ•ˆæ•°æ®ååœæ­¢")
        
        payloads = []
        count = 0
        total_lines = 0
        
        with open(dataset_file, 'r', encoding='utf-8', errors='ignore') as f:
            for line in tqdm(f, desc="  é‡‡æ ·ä¸­", unit=" lines", total=sample_size):
                total_lines += 1
                line = line.strip()
                
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
                if not line or line.startswith('#'):
                    continue
                
                payloads.append(line)
                count += 1
                
                # è¾¾åˆ°ç›®æ ‡é‡‡æ ·æ•°é‡ååœæ­¢
                if count >= sample_size:
                    print(f"\n  â¹ï¸  å·²é‡‡æ · {sample_size:,} æ¡ï¼Œåœæ­¢è¯»å–")
                    break
        
        print(f"âœ… é‡‡æ ·å®Œæˆ: è¯»å– {total_lines:,} è¡Œï¼Œé‡‡æ · {len(payloads):,} æ¡æœ‰æ•ˆæ•°æ®")
        return payloads
    
    def clean_payloads(self, payloads: List[str], attack_type: str) -> List[str]:
        """æ¸…æ´—payloadæ•°æ®"""
        print(f"\nğŸ§¹ æ¸…æ´— {attack_type.upper()} payloads...")
        print(f"  è¾“å…¥æ•°æ®é‡: {len(payloads):,} æ¡")
        
        cleaned = []
        for payload in payloads:
            # å»é™¤è¿‡é•¿æˆ–è¿‡çŸ­çš„payload
            if 5 <= len(payload) <= 500:
                # å»é™¤æ˜æ˜¾çš„æ— æ•ˆpayload
                if not payload.startswith('http://') and not payload.startswith('https://'):
                    cleaned.append(payload)
        
        print(f"  è¿‡æ»¤å: {len(cleaned):,} æ¡")
        
        # å»é‡
        cleaned = list(set(cleaned))
        print(f"  å»é‡å: {len(cleaned):,} æ¡å”¯ä¸€payloads")
        
        # æ ¹æ®è®ºæ–‡è¦æ±‚é™åˆ¶æ•°æ®é‡
        max_size = self.max_samples.get(attack_type.lower())
        if max_size is not None and len(cleaned) > max_size:
            print(f"  ğŸ“‰ æœ€ç»ˆé‡‡æ ·åˆ° {max_size:,} æ¡ï¼ˆè®ºæ–‡è¦æ±‚ï¼‰")
            random.seed(42)
            cleaned = random.sample(cleaned, max_size)
        elif max_size is not None:
            print(f"  âœ… æ•°æ®é‡ {len(cleaned):,} ç¬¦åˆè®ºæ–‡è¦æ±‚ (<= {max_size:,})")
        
        print(f"âœ… æœ€ç»ˆæ•°æ®é‡: {len(cleaned):,} æ¡")
        return cleaned
    
    def split_dataset(self, payloads: List[str], 
                     train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,
                     seed=42) -> Dict[str, List[str]]:
        """åˆ†å‰²æ•°æ®é›†"""
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6
        
        random.seed(seed)
        random.shuffle(payloads)
        
        total = len(payloads)
        train_size = int(total * train_ratio)
        val_size = int(total * val_ratio)
        
        splits = {
            "train": payloads[:train_size],
            "val": payloads[train_size:train_size + val_size],
            "test": payloads[train_size + val_size:]
        }
        
        print(f"\nğŸ“Š æ•°æ®é›†åˆ†å‰²:")
        print(f"  - è®­ç»ƒé›†: {len(splits['train'])} ({train_ratio*100:.0f}%)")
        print(f"  - éªŒè¯é›†: {len(splits['val'])} ({val_ratio*100:.0f}%)")
        print(f"  - æµ‹è¯•é›†: {len(splits['test'])} ({test_ratio*100:.0f}%)")
        
        return splits
    
    def save_dataset(self, splits: Dict[str, List[str]], attack_type: str):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®é›†"""
        attack_dir = self.output_dir / attack_type
        attack_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ’¾ ä¿å­˜æ•°æ®é›†åˆ° {attack_dir}...")
        
        for split_name, payloads in splits.items():
            # ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶
            txt_file = attack_dir / f"{split_name}.txt"
            with open(txt_file, 'w', encoding='utf-8') as f:
                for payload in payloads:
                    f.write(payload + '\n')
            
            # ä¿å­˜ä¸ºJSONæ–‡ä»¶(åŒ…å«å…ƒæ•°æ®)
            json_file = attack_dir / f"{split_name}.json"
            data = {
                "attack_type": attack_type,
                "split": split_name,
                "count": len(payloads),
                "payloads": payloads
            }
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            print(f"  âœ… {split_name}: {txt_file}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = attack_dir / "stats.json"
        stats = {
            "attack_type": attack_type,
            "total": sum(len(p) for p in splits.values()),
            "train": len(splits["train"]),
            "val": len(splits["val"]),
            "test": len(splits["test"])
        }
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2)
        
        print(f"  âœ… stats: {stats_file}")
    
    def prepare_attack_type(self, attack_type: str):
        """å‡†å¤‡æŒ‡å®šæ”»å‡»ç±»å‹çš„æ•°æ®"""
        print("\n" + "="*60)
        print(f"å‡†å¤‡ {attack_type.upper()} æ•°æ®é›†")
        max_size = self.max_samples.get(attack_type.lower())
        if max_size:
            print(f"è®ºæ–‡è¦æ±‚æ•°æ®é‡: {max_size:,} æ¡")
        else:
            print(f"è®ºæ–‡è¦æ±‚: ä½¿ç”¨å…¨é‡æ•°æ®")
        print("="*60)
        
        # åŠ è½½æ•°æ®
        payloads = self.load_dataset(attack_type)
        if not payloads:
            print(f"âŒ æ— æ³•åŠ è½½ {attack_type} æ•°æ®é›†")
            return False
        
        # æ¸…æ´—æ•°æ®
        payloads = self.clean_payloads(payloads, attack_type)
        
        # åˆ†å‰²æ•°æ®
        splits = self.split_dataset(payloads)
        
        # ä¿å­˜æ•°æ®
        self.save_dataset(splits, attack_type)
        
        print(f"\nâœ… {attack_type.upper()} æ•°æ®é›†å‡†å¤‡å®Œæˆ!")
        return True
    
    def prepare_all(self):
        """å‡†å¤‡æ‰€æœ‰æ•°æ®é›†"""
        print("\n" + "="*60)
        print("å‡†å¤‡æ‰€æœ‰æ•°æ®é›†")
        print("="*60)
        
        success_count = 0
        for attack_type in ["sqli", "xss", "rce"]:
            if self.prepare_attack_type(attack_type):
                success_count += 1
        
        print("\n" + "="*60)
        print(f"æ•°æ®å‡†å¤‡å®Œæˆ: {success_count}/3 æˆåŠŸ")
        print("="*60)
        
        # ç”Ÿæˆæ€»ä½“ç»Ÿè®¡
        self.generate_overall_stats()
    
    def generate_overall_stats(self):
        """ç”Ÿæˆæ€»ä½“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for attack_type in ["sqli", "xss", "rce"]:
            stats_file = self.output_dir / attack_type / "stats.json"
            if stats_file.exists():
                with open(stats_file, 'r') as f:
                    stats[attack_type] = json.load(f)
        
        overall_file = self.output_dir / "overall_stats.json"
        with open(overall_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2)
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡: {overall_file}")
        for attack_type, s in stats.items():
            print(f"  {attack_type.upper()}: {s['total']} æ¡payloads")


def main():
    parser = argparse.ArgumentParser(description="æ•°æ®é›†å‡†å¤‡å·¥å…· (æŒ‰GPTFuzzerè®ºæ–‡è¦æ±‚)")
    parser.add_argument("--extract", action="store_true", help="è§£å‹SQLiæ•°æ®é›†")
    parser.add_argument("--attack-type", type=str, choices=["sqli", "xss", "rce", "all"],
                       default="all", help="å‡†å¤‡å“ªä¸ªæ”»å‡»ç±»å‹çš„æ•°æ®")
    parser.add_argument("--data-dir", type=str, default="gptfuzzer-main/Datasets",
                       help="åŸå§‹æ•°æ®ç›®å½•")
    parser.add_argument("--output-dir", type=str, default="data/processed",
                       help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    print("\n" + "="*60)
    print("GPTFuzzer æ•°æ®å‡†å¤‡å·¥å…·")
    print("="*60)
    print("è®ºæ–‡è¦æ±‚çš„æ•°æ®é‡:")
    print("  - SQLi: 512,000 æ¡")
    print("  - XSS:  512,000 æ¡")
    print("  - RCE:  37,302 æ¡ (å…¨é‡)")
    print("="*60)
    
    preparer = DatasetPreparer(args.data_dir, args.output_dir)
    
    # è§£å‹SQLiæ•°æ®é›†
    if args.extract:
        preparer.extract_sqli_dataset()
        return
    
    # å‡†å¤‡æ•°æ®
    if args.attack_type == "all":
        preparer.prepare_all()
    else:
        preparer.prepare_attack_type(args.attack_type)


if __name__ == "__main__":
    main()
