根据论文内容，GPTFuzzer 的预训练（Pre-training）旨在让模型学习攻击载荷的**统计特性**和**语法逻辑**。以下是复现预训练过程的具体技术细节：

### **1\. 模型架构 (Model Architecture)**

* **基础模型**：采用 **GPT-2** 架构的解码器（Decoder-only Transformer）。  
* **参数规模**：12 层 Transformer 层，768 个隐层单元（Hidden units），12 个自注意力头。  
* **初始化**：通常使用随机初始化（针对攻击语法进行从零学习），而非加载通用的 GPT-2 预训练权重，因为攻击载荷的语法（如 SQL/XSS）与自然语言差异巨大。

### **2\. 输入表示 (Input Representation)**

在复现时，你需要将处理好的数据转换为特定的输入格式：

* **序列格式**：每一个攻击载荷序列 $\tau$ 必须以特殊的起始符 `<start>` 开始，并以结束符 `<end>` 结尾。  
* **输入表示**：对于每个时刻 $t$ 的 token $x_t$，其输入由 **Token Embedding**（词嵌入）和 **Positional Embedding**（位置编码）相加而成。

### **3\. 预训练目标 (Pre-training Objective)**

预训练采用标准的**自回归语言建模（Autoregressive Language Modeling）**。

* **核心逻辑**：给定前 $t-1$ 个 token，预测第 $t$ 个 token。  
* 损失函数（$L_1$）：使用交叉熵损失（Cross-Entropy Loss），公式为：

\[
L_1(\theta) = -\sum_{t=1}^{T} \log P(x_t \mid x_0, \dots, x_{t-1}; \theta)
\]

  其中 $\theta$ 是模型的参数，$x_0$ 是起始标记。

### **4\. 复现时的训练超参数建议**

虽然论文在 RL 阶段给出了详细参数，但在预训练阶段通常遵循以下通用设置（结合论文描述）：

* **优化器**：Adam 或 AdamW。  
* **批次大小 (Batch Size)**：建议设置为 64 或 128（取决于显存，论文实验中提到预训练数据量很大）。  
* **数据量选择**：  
  * **SQLi/XSS**：建议使用 **512,000 (512K)** 条有效载荷（这是论文达到最优效果的数据量）。  
  * **RCE**：使用约 **37,000** 条数据。  
* **训练轮数 (Epochs)**：持续训练直到损失函数 $L_1$ 收敛。

### **5\. 两种预训练变体（复现选择）**

在复现时，你可以根据数据情况选择一种模式：

1. **GPTFuzzer-S (Short sequence)**：仅使用攻击载荷的**终结符**（Terminal symbols）进行训练。这适用于你拥有大量现成的真实攻击样本的情况。  
2. **GPTFuzzer-L (Long sequence)**：使用包含**非终结符**（如语法树节点）的完整序列。如果你是基于 EBNF 语法生成数据，这种方式能让模型更好地理解语法结构。

### **6\. 实验环境建议**

* **硬件**：论文虽然没明确预训练显卡，但提到 RL 阶段使用了 **NVIDIA GeForce RTX 3090**（24GB 显存）。考虑到你提到有 **RTX 4070**，运行 1.7B 以下规模的模型预训练是完全可行的。  
* **关键点**：预训练结束后，必须保存模型权重。这个预训练好的模型将作为后续强化学习阶段的 **策略网络（Policy Network）** 的初始版本。

**复现提示：** 论文强调预训练是非常必要的。如果不进行预训练直接进入 RL，模型在生成复杂的 SQL 或 XSS 结构时会表现得非常随机且低效。