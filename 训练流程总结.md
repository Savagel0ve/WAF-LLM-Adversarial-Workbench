# GPTFuzzer è®­ç»ƒæµç¨‹æ€»ç»“

æœ¬æ–‡æ¡£æ€»ç»“åŸºäº GPTFuzzer è®ºæ–‡çš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¥–åŠ±æ¨¡å‹è®­ç»ƒå’Œå¼ºåŒ–å­¦ä¹ ä¸‰ä¸ªé˜¶æ®µã€‚

---

## ğŸ“‹ ç›®å½•

- [æ•´ä½“æ¶æ„](#æ•´ä½“æ¶æ„)
- [é˜¶æ®µä¸€ï¼šé¢„è®­ç»ƒ](#é˜¶æ®µä¸€é¢„è®­ç»ƒ)
- [é˜¶æ®µäºŒï¼šå¥–åŠ±æ¨¡å‹è®­ç»ƒ](#é˜¶æ®µäºŒå¥–åŠ±æ¨¡å‹è®­ç»ƒ)
- [é˜¶æ®µä¸‰ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆPPOï¼‰](#é˜¶æ®µä¸‰å¼ºåŒ–å­¦ä¹ ppo)
- [å®Œæ•´è®­ç»ƒæµç¨‹](#å®Œæ•´è®­ç»ƒæµç¨‹)
- [æ–‡ä»¶ç»“æ„](#æ–‡ä»¶ç»“æ„)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## æ•´ä½“æ¶æ„

GPTFuzzer æ˜¯ä¸€ä¸ªåŸºäº GPT-2 çš„ WAF ç»•è¿‡ payload ç”Ÿæˆæ¡†æ¶ï¼Œé‡‡ç”¨ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       GPTFuzzer ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Stage 1       â”‚    â”‚   Stage 2       â”‚    â”‚   Stage 3       â”‚     â”‚
â”‚  â”‚   é¢„è®­ç»ƒ        â”‚ => â”‚   å¥–åŠ±æ¨¡å‹      â”‚ => â”‚   å¼ºåŒ–å­¦ä¹       â”‚     â”‚
â”‚  â”‚   (Pretrain)    â”‚    â”‚  (Reward Model) â”‚    â”‚   (PPO)         â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                       â”‚                       â”‚               â”‚
â”‚         â–¼                       â–¼                       â–¼               â”‚
â”‚  å­¦ä¹  payload è¯­æ³•       å­¦ä¹  WAF åˆ¤æ–­          ä¼˜åŒ–ç»•è¿‡èƒ½åŠ›           â”‚
â”‚  (ä¸‹ä¸€ä¸ª token é¢„æµ‹)     (åˆ†ç±»: ç»•è¿‡/æ‹¦æˆª)      (æœ€å¤§åŒ–å¥–åŠ±)           â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒæ€æƒ³

1. **é¢„è®­ç»ƒ**ï¼šè®©æ¨¡å‹å­¦ä¹ æ”»å‡» payload çš„è¯­æ³•ç»“æ„ï¼ˆå¦‚ SQL è¯­æ³•ï¼‰
2. **å¥–åŠ±æ¨¡å‹**ï¼šè®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼Œé¢„æµ‹ payload ç»•è¿‡ WAF çš„æ¦‚ç‡
3. **å¼ºåŒ–å­¦ä¹ **ï¼šä½¿ç”¨ PPO ç®—æ³•ï¼Œç»“åˆå¥–åŠ±æ¨¡å‹åé¦ˆï¼Œä¼˜åŒ–æ¨¡å‹ç”Ÿæˆç»•è¿‡ WAF çš„ payload

---

## é˜¶æ®µä¸€ï¼šé¢„è®­ç»ƒ

### ç›®æ ‡

è®© GPT-2 æ¨¡å‹å­¦ä¹ æ”»å‡» payload çš„**ç»Ÿè®¡ç‰¹æ€§**å’Œ**è¯­æ³•é€»è¾‘**ã€‚

### æŠ€æœ¯ç»†èŠ‚

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **æ¨¡å‹æ¶æ„** | GPT-2 (Decoder-only Transformer) |
| **å‚æ•°è§„æ¨¡** | 12 å±‚ï¼Œ768 éšè—å•å…ƒï¼Œ12 æ³¨æ„åŠ›å¤´ (~124M å‚æ•°) |
| **è®­ç»ƒç›®æ ‡** | è‡ªå›å½’è¯­è¨€å»ºæ¨¡ (Causal LM) |
| **æŸå¤±å‡½æ•°** | äº¤å‰ç†µæŸå¤± (Cross-Entropy Loss) |
| **åºåˆ—æ ¼å¼** | `<start>` payload `<end>` |

### æŸå¤±å‡½æ•°

$$L_1(\theta) = -\sum_{t=1}^{T} \log P(x_t \mid x_0, \dots, x_{t-1}; \theta)$$

### è®­ç»ƒè¶…å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `batch_size` | 4 (8GB æ˜¾å­˜) | å¯é€šè¿‡æ¢¯åº¦ç´¯ç§¯ç­‰æ•ˆå¢å¤§ |
| `gradient_accumulation` | 8 | ç­‰æ•ˆ batch_size = 32 |
| `learning_rate` | 5e-5 | æ ‡å‡† fine-tuning å­¦ä¹ ç‡ |
| `epochs` | 5 | è®­ç»ƒè½®æ•° |
| `max_length` | 128 | æœ€å¤§åºåˆ—é•¿åº¦ |
| `fp16` | True | æ··åˆç²¾åº¦è®­ç»ƒ |

### æ•°æ®é‡å»ºè®®ï¼ˆè®ºæ–‡è®¾å®šï¼‰

| æ”»å‡»ç±»å‹ | æ¨èæ•°æ®é‡ |
|----------|-----------|
| SQLi | 512,000 æ¡ |
| XSS | 512,000 æ¡ |
| RCE | 37,000 æ¡ |

### ä½¿ç”¨æ–¹æ³•

```powershell
# ä¸€é”®è®­ç»ƒï¼ˆæ¨èï¼‰
.\train_sqli.ps1

# æˆ–æ‰‹åŠ¨æ‰§è¡Œ
python train\pretrain.py `
    --attack-type sqli `
    --data-dir .\data\processed `
    --output-dir .\models\pretrain_sqli_gpt2_small `
    --model-name gpt2 `
    --epochs 5 `
    --batch-size 4 `
    --gradient-accumulation 8 `
    --fp16
```

### è¾“å‡º

```
models/
â””â”€â”€ pretrain_sqli_gpt2_small/
    â”œâ”€â”€ config.json           # æ¨¡å‹é…ç½®
    â”œâ”€â”€ pytorch_model.bin     # æ¨¡å‹æƒé‡
    â”œâ”€â”€ tokenizer_config.json # Tokenizer é…ç½®
    â”œâ”€â”€ vocab.json            # è¯æ±‡è¡¨
    â”œâ”€â”€ merges.txt            # BPE åˆå¹¶è§„åˆ™
    â””â”€â”€ logs/                 # TensorBoard æ—¥å¿—
```

---

## é˜¶æ®µäºŒï¼šå¥–åŠ±æ¨¡å‹è®­ç»ƒ

### ç›®æ ‡

è®­ç»ƒä¸€ä¸ªåºåˆ—åˆ†ç±»æ¨¡å‹ï¼Œé¢„æµ‹ payload **ç»•è¿‡ WAF çš„æ¦‚ç‡** $r(\tau) \in [0, 1]$ã€‚

### æŠ€æœ¯ç»†èŠ‚

| é¡¹ç›® | è¯´æ˜ |
|------|------|
| **åŸºç¡€æ¨¡å‹** | é¢„è®­ç»ƒçš„ GPT-2 æ¨¡å‹ |
| **æ¶æ„ä¿®æ”¹** | æ·»åŠ åˆ†ç±»å¤´ (Linear Layer + Sigmoid) |
| **è¾“å…¥** | Payload æ–‡æœ¬ |
| **è¾“å‡º** | ç»•è¿‡æ¦‚ç‡ [0, 1] |
| **æŸå¤±å‡½æ•°** | BCEWithLogitsLoss |

### æ¨¡å‹æ¶æ„

```
GPT-2 Transformer (é¢„è®­ç»ƒæƒé‡)
         â†“
[EOS] Token çš„éšè—çŠ¶æ€
         â†“
Linear Layer (768 â†’ 1)
         â†“
BCEWithLogitsLoss
         â†“
Sigmoid â†’ æ¦‚ç‡ [0, 1]
```

### è®­ç»ƒæµç¨‹

```
ã€æ­¥éª¤1ã€‘ç”Ÿæˆæ ‡è®°æ•°æ® (generate_labeled_data.py)
    â”œâ”€ ä»é¢„å¤„ç†æ•°æ®ä¸­é‡‡æ ·
    â”œâ”€ å‘é€åˆ°çœŸå® WAF æµ‹è¯•
    â”œâ”€ æ ¹æ® WAF å“åº”æ‰“æ ‡ç­¾ (200=ç»•è¿‡, 403=æ‹¦æˆª)
    â””â”€ ä¿å­˜ä¸º CSV æ ¼å¼
         â†“
ã€æ­¥éª¤2ã€‘è®­ç»ƒåˆ†ç±»æ¨¡å‹ (train_reward_model.py)
    â”œâ”€ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    â”œâ”€ æ›¿æ¢/æ·»åŠ åˆ†ç±»å¤´
    â”œâ”€ è®­ç»ƒ 4 epochs
    â””â”€ ä¿å­˜å¥–åŠ±æ¨¡å‹
```

### è®­ç»ƒè¶…å‚æ•°ï¼ˆè®ºæ–‡è®¾å®šï¼‰

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `epochs` | 4 | è®­ç»ƒè½®æ•° |
| `batch_size` | 32 | æ‰¹æ¬¡å¤§å° |
| `learning_rate` | 2e-5 | å­¦ä¹ ç‡ |
| `warmup_ratio` | 0.1 | é¢„çƒ­æ¯”ä¾‹ |
| `weight_decay` | 0.01 | æƒé‡è¡°å‡ |
| `max_length` | 128 | æœ€å¤§åºåˆ—é•¿åº¦ |

### æ•°æ®é‡è®¾å®šï¼ˆè®ºæ–‡ï¼‰

| æ”»å‡»ç±»å‹ | æ ‡è®°æ•°æ®é‡ |
|----------|-----------|
| SQLi | 4,000 æ¡ |
| XSS | 2,000 æ¡ |
| RCE | 2,000 æ¡ |

### è¯„ä¼°æŒ‡æ ‡ç›®æ ‡

| æŒ‡æ ‡ | SQLi ç›®æ ‡ | XSS ç›®æ ‡ |
|------|-----------|----------|
| AUC | > 99% | > 98% |
| F1-Score | > 95% | > 95% |
| Accuracy | > 95% | > 95% |

### ä½¿ç”¨æ–¹æ³•

```powershell
# ä¸€é”®è®­ç»ƒï¼ˆæ¨èï¼‰
.\train_reward_sqli.ps1

# æˆ–åˆ†æ­¥æ‰§è¡Œ
# æ­¥éª¤1: ç”Ÿæˆæ ‡è®°æ•°æ®
python train\generate_labeled_data.py `
    --attack_type sqli `
    --input_file .\data\processed\sqli\train.txt `
    --output_dir .\data\labeled `
    --num_samples 4000 `
    --waf_url http://localhost:8081

# æ­¥éª¤2: è®­ç»ƒå¥–åŠ±æ¨¡å‹
python train\train_reward_model.py `
    --pretrained_model_path .\models\pretrain_sqli_gpt2_small `
    --data_path .\data\labeled `
    --output_dir .\models\reward_sqli `
    --batch_size 32 `
    --epochs 4 `
    --fp16
```

### è¾“å‡º

```
data/labeled/
â”œâ”€â”€ sqli_train.csv          # è®­ç»ƒæ•°æ® (70%)
â”œâ”€â”€ sqli_val.csv            # éªŒè¯æ•°æ® (15%)
â”œâ”€â”€ sqli_test.csv           # æµ‹è¯•æ•°æ® (15%)
â””â”€â”€ sqli_stats.json         # æ•°æ®ç»Ÿè®¡

models/reward_sqli/
â”œâ”€â”€ final_reward_model/     # ğŸ¯ æœ€ç»ˆæ¨¡å‹ï¼ˆç”¨äº PPOï¼‰
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ tokenizer_config.json
â”œâ”€â”€ checkpoint-*/           # è®­ç»ƒæ£€æŸ¥ç‚¹
â”œâ”€â”€ logs/                   # TensorBoard æ—¥å¿—
â””â”€â”€ test_results.json       # æµ‹è¯•ç»“æœ
```

---

## é˜¶æ®µä¸‰ï¼šå¼ºåŒ–å­¦ä¹ ï¼ˆPPOï¼‰

### ç›®æ ‡

ä½¿ç”¨ PPO ç®—æ³•å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆ**èƒ½å¤Ÿç»•è¿‡ WAF çš„ payload**ã€‚

### æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | è¯´æ˜ |
|------|------|
| **Policy Network** | ä»é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–ï¼Œç”Ÿæˆ payload |
| **Reference Model** | å†»ç»“çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œè®¡ç®— KL æ•£åº¦ |
| **Reward Model** | é˜¶æ®µäºŒè®­ç»ƒçš„åˆ†ç±»å™¨ï¼Œè¯„ä¼°ç»•è¿‡æ¦‚ç‡ |
| **PPO ç®—æ³•** | åœ¨å¥–åŠ±å’Œ KL æ•£åº¦ä¹‹é—´å¹³è¡¡ |

### å¥–åŠ±å‡½æ•°ï¼ˆè®ºæ–‡æ ¸å¿ƒï¼‰

$$R_{total} = R_{WAF} - \beta \cdot KL(\pi_\theta, \rho)$$

| ç¬¦å· | å«ä¹‰ |
|------|------|
| $R_{WAF}$ | å¥–åŠ±æ¨¡å‹è¾“å‡ºçš„ç»•è¿‡æ¦‚ç‡ (0-1) |
| $KL(\pi_\theta, \rho)$ | å½“å‰ç­–ç•¥ä¸é¢„è®­ç»ƒæ¨¡å‹çš„ KL æ•£åº¦ |
| $\beta = 0.2$ | KL ç³»æ•°ï¼Œå¹³è¡¡ç»•è¿‡ç‡å’Œè¯­æ³•æ­£ç¡®æ€§ |

### è®­ç»ƒè¶…å‚æ•°ï¼ˆè®ºæ–‡è®¾å®šï¼‰

| å‚æ•° | è®ºæ–‡æ¨èå€¼ | è¯´æ˜ |
|------|-----------|------|
| `learning_rate` | **1.4e-5** | éå¸¸å°ï¼Œé¿å…å¿˜è®°è¯­æ³• |
| `batch_size` | **256** | æ¯è½®ç”Ÿæˆçš„æ ·æœ¬æ•° |
| `mini_batch_size` | **16** | PPO æ›´æ–°çš„ mini batch |
| `init_kl_coef` (Î²) | **0.2** | ğŸ”¥ **æœ€å…³é”®å‚æ•°** |
| `ppo_epochs` | **4** | PPO å†…éƒ¨æ›´æ–°æ¬¡æ•° |
| `total_episodes` | **20** | æ€»è®­ç»ƒè½®æ•° |

### PPO å·¥ä½œæµç¨‹

```
æ¯ä¸ª Episode:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ğŸ² Generate (Rollout)                                          â”‚
â”‚    ä½¿ç”¨å½“å‰ç­–ç•¥ç½‘ç»œç”Ÿæˆ batch_size ä¸ªè½½è·                          â”‚
â”‚                          â†“                                         â”‚
â”‚ 2. ğŸ Calculate Rewards                                           â”‚
â”‚    å¥–åŠ±æ¨¡å‹è¯„ä¼°æ¯ä¸ªè½½è·çš„ WAF ç»•è¿‡æ¦‚ç‡ (0-1)                        â”‚
â”‚                          â†“                                         â”‚
â”‚ 3. ğŸ”„ PPO Update                                                  â”‚
â”‚    æ ¹æ®å¥–åŠ±å’Œ KL æ•£åº¦æ›´æ–°ç­–ç•¥ç½‘ç»œå‚æ•°                              â”‚
â”‚                          â†“                                         â”‚
â”‚ 4. ğŸ’¾ Save Checkpoint                                             â”‚
â”‚    å®šæœŸä¿å­˜æ¨¡å‹                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ”¶æ•›æ ‡å‡†

| é˜¶æ®µ | Episodes | å¹³å‡å¥–åŠ± | è¯´æ˜ |
|------|----------|---------|------|
| åˆæœŸ | 1-5 | 0.01 ~ 0.1 | æ¨¡å‹åœ¨æ¢ç´¢ |
| ä¸­æœŸ | 6-15 | 0.1 ~ 0.3 | æ‰¾åˆ°æœ‰æ•ˆæ¨¡å¼ |
| æ”¶æ•› | 16-20 | 0.3 ~ 0.5+ | å¥–åŠ±ç¨³å®š |

### ä½¿ç”¨æ–¹æ³•

```powershell
# ä¸€é”®è®­ç»ƒï¼ˆæ¨èï¼‰
.\train_rl_sqli.ps1

# æˆ–æ‰‹åŠ¨æ‰§è¡Œ
python train\train_rl.py `
    --pretrained_model .\models\pretrain_sqli_gpt2_small `
    --reward_model .\models\reward_sqli\final_reward_model `
    --output_dir .\models\rl_sqli_gpt2 `
    --total_episodes 20 `
    --batch_size 256 `
    --mini_batch_size 16 `
    --init_kl_coef 0.2 `
    --learning_rate 1.4e-5
```

### è¾“å‡º

```
models/rl_sqli_gpt2/
â”œâ”€â”€ checkpoint-5/              # ç¬¬ 5 è½®æ£€æŸ¥ç‚¹
â”œâ”€â”€ checkpoint-10/             # ç¬¬ 10 è½®æ£€æŸ¥ç‚¹
â”œâ”€â”€ checkpoint-15/             # ç¬¬ 15 è½®æ£€æŸ¥ç‚¹
â”œâ”€â”€ checkpoint-20/             # ç¬¬ 20 è½®æ£€æŸ¥ç‚¹
â”œâ”€â”€ final_model/               # ğŸ¯ æœ€ç»ˆæ¨¡å‹
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ rl_config.json
â”œâ”€â”€ training_stats.json        # è®­ç»ƒç»Ÿè®¡
â””â”€â”€ rl_config.json             # è®­ç»ƒé…ç½®
```

---

## å®Œæ•´è®­ç»ƒæµç¨‹

### å‰ç½®å‡†å¤‡

1. **ç¯å¢ƒé…ç½®**
   ```powershell
   # å®‰è£…ä¾èµ–
   pip install -r train\requirements_train.txt
   
   # å¯åŠ¨ WAF æœåŠ¡ï¼ˆç”¨äºå¥–åŠ±æ¨¡å‹è®­ç»ƒï¼‰
   cd waf-bench
   docker-compose up -d
   ```

2. **æ•°æ®å‡†å¤‡**
   ```powershell
   # å‡†å¤‡è®­ç»ƒæ•°æ®
   python train\prepare_data.py --attack_type sqli
   ```

### å®Œæ•´è®­ç»ƒå‘½ä»¤ (Qwen2.5-Coder)

```powershell
# ================================
# ä¸€é”®å®Œæ•´è®­ç»ƒ (æ¨è)
# ================================
.\train_sqli_qwen.ps1

# ================================
# æˆ–åˆ†é˜¶æ®µæ‰§è¡Œ:
# ================================

# Stage 1: é¢„è®­ç»ƒ
python train\pretrain.py `
    --model-preset qwen2.5-coder-1.5b `
    --attack-type sqli `
    --epochs 3 `
    --bf16

# Stage 2: å¥–åŠ±æ¨¡å‹è®­ç»ƒ
# 2.1 æµ‹è¯• WAF è¿æ¥
.\test_reward_waf.ps1

# 2.2 ç”Ÿæˆæ ‡è®°æ•°æ®
python train\generate_labeled_data.py `
    --attack_type sqli `
    --input_file .\data\processed\sqli\train.txt `
    --output_dir .\data\labeled `
    --num_samples 4000

# 2.3 è®­ç»ƒå¥–åŠ±æ¨¡å‹
python train\train_reward_model.py `
    --pretrained_model_path .\models\pretrain_sqli_qwen2_5_coder_1_5b `
    --data_path .\data\labeled `
    --output_dir .\models\reward_sqli_qwen `
    --bf16

# Stage 3: å¼ºåŒ–å­¦ä¹ 
python train\train_rl.py `
    --pretrained_model .\models\pretrain_sqli_qwen2_5_coder_1_5b `
    --reward_model .\models\reward_sqli_qwen\final_reward_model `
    --output_dir .\models\rl_sqli_qwen `
    --bf16

# ================================
# æµ‹è¯•æœ€ç»ˆæ¨¡å‹
# ================================
python train\test_rl_model.py `
    --model_path .\models\rl_sqli_qwen\final_model `
    --num_samples 50
```

### è®­ç»ƒæ—¶é—´ä¼°è®¡ (Qwen2.5-Coder-1.5B)

| é˜¶æ®µ | RTX 4070 (8GB) | RTX 4090 (24GB) |
|------|---------------|-----------------|
| é¢„è®­ç»ƒ | ~1-2 å°æ—¶ | ~30-60 åˆ†é’Ÿ |
| å¥–åŠ±æ¨¡å‹ | ~20-30 åˆ†é’Ÿ | ~10-15 åˆ†é’Ÿ |
| å¼ºåŒ–å­¦ä¹  | ~2-3 å°æ—¶ | ~1-1.5 å°æ—¶ |

---

## æ–‡ä»¶ç»“æ„

```
WAF-LLM-Adversarial-Workbench/
â”œâ”€â”€ train/                          # è®­ç»ƒè„šæœ¬ç›®å½•
â”‚   â”œâ”€â”€ pretrain.py                 # é¢„è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ generate_labeled_data.py    # æ•°æ®æ ‡è®°è„šæœ¬
â”‚   â”œâ”€â”€ train_reward_model.py       # å¥–åŠ±æ¨¡å‹è®­ç»ƒ
â”‚   â”œâ”€â”€ train_rl.py                 # å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
â”‚   â”œâ”€â”€ test_reward_model.py        # å¥–åŠ±æ¨¡å‹æµ‹è¯•
â”‚   â”œâ”€â”€ test_rl_model.py            # RL æ¨¡å‹æµ‹è¯•
â”‚   â”œâ”€â”€ generate_payloads.py        # æ‰¹é‡ç”Ÿæˆ payload
â”‚   â”œâ”€â”€ evaluate_rl.py              # è¯„ä¼° WAF ç»•è¿‡ç‡
â”‚   â”œâ”€â”€ config.py                   # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ waf_env.py                  # WAF ç¯å¢ƒæ¥å£
â”‚   â”œâ”€â”€ verifier.py                 # Payload éªŒè¯å™¨
â”‚   â””â”€â”€ requirements_train.txt      # ä¾èµ–åˆ—è¡¨
â”‚
â”œâ”€â”€ data/                           # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ processed/                  # é¢„å¤„ç†åçš„æ•°æ®
â”‚   â”‚   â”œâ”€â”€ sqli/
â”‚   â”‚   â”‚   â”œâ”€â”€ train.txt
â”‚   â”‚   â”‚   â”œâ”€â”€ val.txt
â”‚   â”‚   â”‚   â””â”€â”€ test.txt
â”‚   â”‚   â”œâ”€â”€ xss/
â”‚   â”‚   â””â”€â”€ rce/
â”‚   â””â”€â”€ labeled/                    # WAF æ ‡è®°æ•°æ®
â”‚       â”œâ”€â”€ sqli_train.csv
â”‚       â”œâ”€â”€ sqli_val.csv
â”‚       â””â”€â”€ sqli_test.csv
â”‚
â”œâ”€â”€ models/                         # æ¨¡å‹è¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ pretrain_sqli_gpt2_small/   # é¢„è®­ç»ƒæ¨¡å‹
â”‚   â”œâ”€â”€ reward_sqli/                # å¥–åŠ±æ¨¡å‹
â”‚   â”‚   â””â”€â”€ final_reward_model/
â”‚   â””â”€â”€ rl_sqli_gpt2/               # RL å¾®è°ƒæ¨¡å‹
â”‚       â””â”€â”€ final_model/
â”‚
â”œâ”€â”€ waf-bench/                      # WAF æµ‹è¯•ç¯å¢ƒ
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ train_sqli.ps1                  # é¢„è®­ç»ƒä¸€é”®è„šæœ¬
â”œâ”€â”€ train_reward_sqli.ps1           # å¥–åŠ±æ¨¡å‹ä¸€é”®è„šæœ¬
â”œâ”€â”€ train_rl_sqli.ps1               # RL ä¸€é”®è„šæœ¬
â”‚
â”œâ”€â”€ QUICKSTART_REWARD.md            # å¥–åŠ±æ¨¡å‹å¿«é€Ÿå…¥é—¨
â”œâ”€â”€ QUICKSTART_RL.md                # RL å¿«é€Ÿå…¥é—¨
â”œâ”€â”€ README_REWARD_MODEL.md          # å¥–åŠ±æ¨¡å‹è¯¦ç»†æ–‡æ¡£
â”œâ”€â”€ README_RL.md                    # RL è¯¦ç»†æ–‡æ¡£
â””â”€â”€ è®­ç»ƒæµç¨‹æ€»ç»“.md                  # æœ¬æ–‡æ¡£
```

---

## å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³ (OOM)

**è§£å†³æ–¹æ¡ˆ**ï¼š

```powershell
# é¢„è®­ç»ƒé˜¶æ®µ
python train\pretrain.py --batch-size 2 --gradient-accumulation 16

# å¥–åŠ±æ¨¡å‹é˜¶æ®µ
python train\train_reward_model.py --batch_size 16

# å¼ºåŒ–å­¦ä¹ é˜¶æ®µ
python train\train_rl.py --batch_size 128 --mini_batch_size 8
```

### Q2: WAF è¿æ¥å¤±è´¥

```powershell
# æ£€æŸ¥ WAF çŠ¶æ€
cd waf-bench
docker-compose ps

# é‡å¯ WAF
docker-compose restart

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f
```

### Q3: é¢„è®­ç»ƒ Loss ä¸ä¸‹é™

- æ£€æŸ¥æ•°æ®è´¨é‡ï¼šç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
- é™ä½å­¦ä¹ ç‡ï¼š`--learning-rate 1e-5`
- å¢åŠ è®­ç»ƒè½®æ•°ï¼š`--epochs 10`

### Q4: å¥–åŠ±æ¨¡å‹ AUC ä½

- å¢åŠ æ•°æ®é‡ï¼š`--num_samples 8000`
- æ£€æŸ¥æ•°æ®å¹³è¡¡ï¼šæŸ¥çœ‹ `sqli_stats.json`
- è°ƒæ•´è¶…å‚æ•°ï¼š`--epochs 6 --learning_rate 1e-5`

### Q5: RL è®­ç»ƒå¥–åŠ±ä¸ä¸Šå‡

- **æ£€æŸ¥å¥–åŠ±æ¨¡å‹è´¨é‡**ï¼šæµ‹è¯•é›† AUC > 90%
- **è°ƒæ•´ KL ç³»æ•°**ï¼š`--init_kl_coef 0.1`
- **æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹**ï¼šç¡®ä¿é¢„è®­ç»ƒå……åˆ†

### Q6: ç”Ÿæˆçš„ payload é‡å¤/æ— æ•ˆ

```powershell
# å¢åŠ ç”Ÿæˆéšæœºæ€§
python train\test_rl_model.py --temperature 1.5 --top_k 100

# è°ƒæ•´ KL ç³»æ•°ï¼ˆä¿æŒè¯­æ³•æ­£ç¡®æ€§ï¼‰
python train\train_rl.py --init_kl_coef 0.3
```

---

## æ¨¡å‹å‡çº§æ–¹æ¡ˆ

GPT-2ï¼ˆ2019å¹´å‘å¸ƒï¼Œ124M å‚æ•°ï¼‰å·²ç»æ¯”è¾ƒè€æ—§ã€‚ä»¥ä¸‹æ˜¯å¯ç”¨äºåŒæ ·è®­ç»ƒæµç¨‹çš„æ›´å…ˆè¿›æ¨¡å‹ã€‚

### æ¨èæ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | 8GB æ˜¾å­˜ | ç‰¹ç‚¹ | æ¨èåº¦ |
|------|--------|----------|------|--------|
| **Qwen2.5** | 0.5B/1.5B/3B | âœ… | ä¸­è‹±åŒè¯­ï¼Œä»£ç èƒ½åŠ›å¼º | â­â­â­â­â­ |
| **Phi-3** | 3.8B | âœ… (4-bit) | å¾®è½¯å‡ºå“ï¼Œå°è€Œå¼º | â­â­â­â­â­ |
| **Gemma 2** | 2B/9B | âœ… (2B) | Googleå‡ºå“ï¼Œé«˜æ•ˆ | â­â­â­â­ |
| **Mistral** | 7B | âœ… (4-bit) | è¶…è¶ŠLLaMA2-13B | â­â­â­â­ |
| **LLaMA 3.2** | 1B/3B | âœ… | Metaå‡ºå“ï¼Œç”Ÿæ€å®Œå–„ | â­â­â­â­ |
| **DeepSeek Coder** | 1.3B/6.7B | âœ… (1.3B) | ä»£ç ä¸“ç”¨ï¼Œé€‚åˆpayload | â­â­â­â­â­ |
| **StarCoder2** | 3B/7B | âœ… (3B) | ä»£ç ç”Ÿæˆä¸“ç”¨ | â­â­â­â­ |

### ğŸ¯ é¦–é€‰æ¨èï¼šQwen2.5-1.5B / Qwen2.5-Coder-1.5B

**ç†ç”±**ï¼š
- å‚æ•°é‡é€‚ä¸­ï¼ˆ1.5Bï¼‰ï¼Œ8GB æ˜¾å­˜å¯å…¨ç²¾åº¦è®­ç»ƒ
- ä»£ç å’Œç»“æ„åŒ–æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›å¼º
- ä¸­æ–‡ç¤¾åŒºæ”¯æŒå®Œå–„
- Apache 2.0 è®¸å¯ï¼Œå¯å•†ç”¨

```python
# æ›¿æ¢ GPT-2 ä¸º Qwen2.5
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen2.5-1.5B"  # æˆ– "Qwen/Qwen2.5-Coder-1.5B"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
tokenizer = AutoTokenizer.from_pretrained(model_name)
```

### ğŸ”§ æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯

å¯¹äºè¾ƒå¤§æ¨¡å‹ï¼ˆ7B+ï¼‰ï¼Œéœ€è¦ä½¿ç”¨ä»¥ä¸‹æŠ€æœ¯ï¼š

#### 1. QLoRA å¾®è°ƒï¼ˆæ¨èï¼‰

```python
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from transformers import BitsAndBytesConfig

# 4-bit é‡åŒ–é…ç½®
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

# åŠ è½½ 7B æ¨¡å‹ï¼ˆ4-bit çº¦å ç”¨ ~4GB æ˜¾å­˜ï¼‰
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    quantization_config=bnb_config,
    device_map="auto",
)

# LoRA é…ç½®
lora_config = LoraConfig(
    r=16,                    # LoRA ç§©
    lora_alpha=32,           # ç¼©æ”¾å› å­
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
```

#### 2. æ˜¾å­˜éœ€æ±‚å¯¹æ¯”

| æ¨¡å‹ | å…¨ç²¾åº¦ (FP32) | åŠç²¾åº¦ (FP16) | 4-bit é‡åŒ– | 4-bit + LoRA |
|------|--------------|--------------|-----------|--------------|
| GPT-2 (124M) | 0.5 GB | 0.25 GB | - | - |
| Qwen2.5-1.5B | 6 GB | 3 GB | 1.5 GB | 2 GB |
| Phi-3-mini (3.8B) | 15 GB | 7.5 GB | 2.5 GB | 3.5 GB |
| Mistral-7B | 28 GB | 14 GB | 4 GB | 5-6 GB |
| LLaMA3-8B | 32 GB | 16 GB | 5 GB | 6-7 GB |

### ğŸ“‹ ä¸‰é˜¶æ®µé€‚é…æ–¹æ¡ˆ

#### Stage 1: é¢„è®­ç»ƒé€‚é…

```python
# pretrain.py ä¿®æ”¹
from transformers import AutoModelForCausalLM, AutoTokenizer

def setup_model_and_tokenizer(model_name: str):
    """æ”¯æŒå¤šç§æ¨¡å‹"""
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    
    # è®¾ç½® pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # æ ¹æ®æ˜¾å­˜é€‰æ‹©åŠ è½½æ–¹å¼
    if "7b" in model_name.lower() or "8b" in model_name.lower():
        # å¤§æ¨¡å‹ä½¿ç”¨ 4-bit é‡åŒ–
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            load_in_4bit=True,
            device_map="auto",
            trust_remote_code=True,
        )
    else:
        # å°æ¨¡å‹ä½¿ç”¨ FP16
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
        )
    
    return model, tokenizer

# ä½¿ç”¨ç¤ºä¾‹
model, tokenizer = setup_model_and_tokenizer("Qwen/Qwen2.5-Coder-1.5B")
```

#### Stage 2: å¥–åŠ±æ¨¡å‹é€‚é…

```python
# train_reward_model.py ä¿®æ”¹
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(
    "Qwen/Qwen2.5-1.5B",
    num_labels=1,  # BCEWithLogitsLoss
    torch_dtype=torch.float16,
    trust_remote_code=True,
)
```

#### Stage 3: PPO é€‚é…

```python
# train_rl.py ä¿®æ”¹
from trl import AutoModelForCausalLMWithValueHead

model = AutoModelForCausalLMWithValueHead.from_pretrained(
    "path/to/pretrained_qwen",
    torch_dtype=torch.float16,
    peft_config=lora_config,  # å¯é€‰ï¼šä½¿ç”¨ LoRA
)
```

### ğŸš€ ä»£ç ä¸“ç”¨æ¨¡å‹æ¨è

å¯¹äº WAF payload ç”Ÿæˆï¼Œ**ä»£ç ä¸“ç”¨æ¨¡å‹**å¯èƒ½æ•ˆæœæ›´å¥½ï¼š

| æ¨¡å‹ | å‚æ•°é‡ | ç‰¹ç‚¹ |
|------|--------|------|
| **Qwen2.5-Coder** | 1.5B/7B | ä»£ç èƒ½åŠ›å¼ºï¼Œæ”¯æŒå¤šè¯­è¨€ |
| **DeepSeek-Coder** | 1.3B/6.7B | ä¸“ä¸ºä»£ç è®¾è®¡ï¼Œç†è§£è¯­æ³•å¥½ |
| **StarCoder2** | 3B/7B/15B | BigCode å‡ºå“ï¼Œä»£ç ç”Ÿæˆä¸“ç”¨ |
| **CodeLlama** | 7B/13B | Meta ä»£ç æ¨¡å‹ï¼Œæ”¯æŒå¡«å…… |

### ğŸ“Š æ€§èƒ½å¯¹æ¯”é¢„ä¼°

åŸºäºæ¨¡å‹èƒ½åŠ›ï¼Œé¢„æœŸè®­ç»ƒåçš„ WAF ç»•è¿‡ç‡æå‡ï¼š

| æ¨¡å‹ | é¢„æœŸç»•è¿‡ç‡ | è®­ç»ƒç¨³å®šæ€§ | ç”Ÿæˆå¤šæ ·æ€§ |
|------|-----------|-----------|-----------|
| GPT-2 (124M) | åŸºå‡† | â­â­â­â­â­ | â­â­â­ |
| Qwen2.5-1.5B | +15-25% | â­â­â­â­ | â­â­â­â­ |
| Phi-3-mini | +10-20% | â­â­â­â­ | â­â­â­â­ |
| Mistral-7B (LoRA) | +20-30% | â­â­â­ | â­â­â­â­â­ |
| DeepSeek-Coder-1.3B | +15-25% | â­â­â­â­ | â­â­â­â­ |

### âš¡ å¿«é€Ÿè¿ç§»æ¸…å•

ä» GPT-2 è¿ç§»åˆ°æ–°æ¨¡å‹éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶ï¼š

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ |
|------|----------|
| `train/config.py` | æ·»åŠ æ–°æ¨¡å‹é…ç½® |
| `train/pretrain.py` | ä½¿ç”¨ `AutoModelForCausalLM` |
| `train/train_reward_model.py` | ä½¿ç”¨ `AutoModelForSequenceClassification` |
| `train/train_rl.py` | æ·»åŠ  LoRA æ”¯æŒ |
| `train/requirements_train.txt` | æ·»åŠ  `peft`, `bitsandbytes` |

### ğŸ’¡ æœ€ä½³å®è·µå»ºè®®

1. **8GB æ˜¾å­˜é¦–é€‰**: `Qwen2.5-Coder-1.5B` - æ— éœ€é‡åŒ–ï¼Œè®­ç»ƒç¨³å®š
2. **è¿½æ±‚æ€§èƒ½**: `Mistral-7B` + QLoRA - éœ€è¦ 4-bit é‡åŒ–
3. **å¿«é€Ÿå®éªŒ**: `Phi-3-mini` - å°è€Œå¼ºï¼Œè¿­ä»£å¿«
4. **ä»£ç ä»»åŠ¡**: `DeepSeek-Coder-1.3B` - è¯­æ³•ç†è§£èƒ½åŠ›å¼º

---

## å‚è€ƒèµ„æ–™

- **è®ºæ–‡**: [GPTFuzzer: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts](https://arxiv.org/abs/2309.10253)
- **PPO ç®—æ³•**: [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)
- **TRL åº“**: [Transformer Reinforcement Learning](https://github.com/huggingface/trl)
- **PEFT åº“**: [Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft)
- **Qwen2.5**: [https://huggingface.co/Qwen](https://huggingface.co/Qwen)
- **DeepSeek Coder**: [https://huggingface.co/deepseek-ai](https://huggingface.co/deepseek-ai)

---

## æ€»ç»“

| é˜¶æ®µ | ç›®æ ‡ | æ ¸å¿ƒè¾“å…¥ | æ ¸å¿ƒè¾“å‡º | è®­ç»ƒæ—¶é—´ |
|------|------|----------|----------|----------|
| **é¢„è®­ç»ƒ** | å­¦ä¹  payload è¯­æ³• | åŸå§‹ payload æ•°æ® | è¯­è¨€æ¨¡å‹ | 2-4 å°æ—¶ |
| **å¥–åŠ±æ¨¡å‹** | å­¦ä¹  WAF åˆ¤æ–­ | é¢„è®­ç»ƒæ¨¡å‹ + WAF æ ‡ç­¾ | åˆ†ç±»å™¨ | 15-25 åˆ†é’Ÿ |
| **å¼ºåŒ–å­¦ä¹ ** | ä¼˜åŒ–ç»•è¿‡èƒ½åŠ› | é¢„è®­ç»ƒæ¨¡å‹ + å¥–åŠ±æ¨¡å‹ | æœ€ç»ˆç”Ÿæˆæ¨¡å‹ | 2-4 å°æ—¶ |

**æ ¸å¿ƒåˆ›æ–°**ï¼šé€šè¿‡å¥–åŠ±æ¨¡å‹æä¾›è¿ç»­çš„æ¦‚ç‡ä¿¡å·ï¼ˆè€Œéç®€å•çš„ 0/1 åé¦ˆï¼‰ï¼Œä½¿ PPO ç®—æ³•èƒ½å¤Ÿç¨³å®šåœ°ä¼˜åŒ–ç­–ç•¥ç½‘ç»œï¼Œç”Ÿæˆæ—¢ç¬¦åˆè¯­æ³•åˆèƒ½ç»•è¿‡ WAF çš„ payloadã€‚

---

**ä½œè€…**: WAF-LLM-Adversarial-Workbench  
**æ—¥æœŸ**: 2026-02-03  
**ç‰ˆæœ¬**: 1.0
