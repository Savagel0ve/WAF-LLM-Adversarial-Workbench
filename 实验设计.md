# GPTFuzzer 论文复现实验设计

> 参考论文: "Generative Pre-Trained Transformer-Based Reinforcement Learning for Testing Web Application Firewalls"
> IEEE TDSC 2024

---

## 一、实验概述

### 1.1 研究目标

复现GPTFuzzer论文中的6个研究问题(RQ)实验，验证基于GPT的强化学习方法在WAF测试中的有效性。

### 1.2 实验架构

```
┌─────────────────────────────────────────────────────────────────┐
│                        数据准备阶段                              │
│  SQLi: 512,000条  |  XSS: 512,000条  |  RCE: 37,302条           │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                      三阶段训练流程                              │
│  Stage 1: 预训练 → Stage 2: 奖励模型 → Stage 3: RL训练          │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                      6个研究问题实验                             │
│  RQ1: 有效性  RQ2: 语法  RQ3: 数据规模                          │
│  RQ4: 奖励模型  RQ5: 超参数  RQ6: 新颖性                        │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│                        评估与分析                                │
│  WAF测试 → 指标计算 → 可视化 → 结论                             │
└─────────────────────────────────────────────────────────────────┘
```

---

## 二、评估指标定义

### 2.1 核心指标

| 指标 | 全称 | 定义 | 公式 |
|------|------|------|------|
| **TP** | True Positives | 在给定请求数内发现的绕过payload数量 | TP = count(bypassed) |
| **ER** | Effective Rate | 绕过payload数占生成不重复payload数的比例 | ER = TP_gen / N_distinct |
| **NRR** | Non-Repetition Rate | 不重复payload数占总生成数的比例 | NRR = N_distinct / N_total |
| **TER** | Total Effective Rate | 综合考虑奖励模型训练阶段的总有效率 | TER = (TP_reward + TP_gen) / (\|Dr\| + N_distinct) |

### 2.2 辅助指标

| 指标 | 说明 |
|------|------|
| **TSR** | Time Spent per Request - 生成+测试单个payload的时间 |
| **Valid Rate** | 语法有效率 - 符合攻击语法要求的payload比例 |
| **Functional Rate** | 功能性验证成功率 - 实际能执行攻击的payload比例 |
| **Perplexity** | 语言模型困惑度 - 评估预训练效果 |

---

## 三、实验环境配置

### 3.1 硬件环境

| 环境 | GPU | 显存 | 说明 |
|------|-----|------|------|
| 本地开发 | RTX 4070 | 8GB | Windows, 优化参数 |
| 服务器训练 | RTX 4090 | 24GB | Ubuntu, Flash Attention |

### 3.2 软件环境

```
Python 3.10+
PyTorch 2.0+
Transformers 4.35+
TRL (Transformer Reinforcement Learning)
```

### 3.3 目标WAF

| WAF | 版本 | 规则集 |
|-----|------|--------|
| ModSecurity | 3.0.x | OWASP CRS 3.3.x |
| Naxsi | 1.3 | 内置规则 |

### 3.4 测试应用

- **DVWA** (Damn Vulnerable Web Application)
- SQLi/XSS: HTTP GET 参数
- RCE: HTTP POST 参数

---

## 四、研究问题实验设计

### RQ1: GPTFuzzer是否有效且高效？

#### 实验目标
对比GPTFuzzer与基线方法(Random Fuzzer, Grammar-based RL)的TP和效率。

#### 实验配置

| 参数 | SQLi | XSS | RCE |
|------|------|-----|-----|
| 请求预算 | 1,250,000 | 35,000 | 35,000 |
| 预训练数据 | 512,000 | 512,000 | 37,302 |
| 奖励模型数据 | 4,000 | 2,000 | 2,000 |

#### 对比方法

1. **GPTFuzzer** (本方法)
   - 预训练 + 奖励模型 + PPO强化学习
   
2. **Random Fuzzer** (基线1)
   - 从攻击语法随机采样生成payload
   
3. **Grammar-based RL** (基线2)
   - 不使用语言模型，仅用RL学习语法分支权重

#### 评估流程

```python
# 伪代码
for attack_type in ['sqli', 'xss', 'rce']:
    for waf in ['modsecurity', 'naxsi']:
        for method in ['gptfuzzer', 'random', 'grammar_rl']:
            results = []
            for request_count in checkpoints:
                tp = evaluate(method, attack_type, waf, request_count)
                results.append((request_count, tp))
            save_results(results)
```

#### 预期输出
- TP vs 请求数曲线图 (论文Figure 6风格)
- 各方法最终TP对比表

---

### RQ2: 攻击语法如何影响GPTFuzzer的效果和效率？

#### 实验目标
对比短序列(仅终结符)和长序列(含非终结符)的效果差异。

#### 数据准备

| 序列类型 | 说明 | 示例 |
|----------|------|------|
| 短序列 | 只包含终结符 | `0 %0b and %0b not ~ false %23` |
| 长序列 | 包含所有符号 | `<start> <numericCtx> 0 <booleanAtk> ...` |

#### 实验配置

```python
RQ2_CONFIG = {
    "sequence_types": ["short", "long"],
    "attack_types": ["sqli", "xss", "rce"],
    "metrics": ["tp", "tsr", "generation_time"]
}
```

#### 评估指标
- TP (绕过数量)
- TSR (每请求时间)
- 平均序列长度
- 生成速度 (tokens/sec)

#### 预期结论
- 长序列可能略微提高TP
- 但生成时间增加约3倍
- 存在质量与速度的权衡

---

### RQ3: 不同预训练数据规模对GPTFuzzer有何影响？

#### 实验目标
探索预训练数据量对最终效果的影响。

#### 数据规模设置

| 设置 | 数据量 | 说明 |
|------|--------|------|
| 无预训练 | 0K | 直接进行RL |
| 小规模 | 20K | 最小可用 |
| 中规模 | 256K | 论文一半 |
| 全规模 | 512K | 论文配置 |

#### 实验流程

```
对于每种数据规模:
  1. 预训练语言模型
  2. 训练奖励模型
  3. RL微调
  4. 评估TP和语法有效率
```

#### 评估指标
- TP (绕过数量)
- Valid Rate (语法有效率)
- Perplexity (预训练困惑度)

#### 预期结论
- 0K: 性能很差，无法生成有效payload
- 20K: 仍有0.1%-1%无效payload
- 256K-512K: 达到最优效果

---

### RQ4: 奖励模型vs直接WAF反馈有什么优势？

#### 实验目标
对比奖励模型指导RL与直接使用WAF二元反馈的效果。

#### 实验设置

| 方法 | 奖励来源 | 奖励值 |
|------|----------|--------|
| 奖励模型 | 预测的绕过概率 | r(τ) ∈ [0, 1] |
| WAF反馈 | 实际测试结果 | 0 或 1 |

#### 实验流程

```python
# 奖励模型指导
reward_model_results = train_rl_with_reward_model()

# WAF直接反馈
waf_guided_results = train_rl_with_waf_feedback()

# 对比TP曲线
compare_tp_curves(reward_model_results, waf_guided_results)
```

#### 评估指标
- TP随请求数的变化曲线
- 收敛速度
- 最终绕过率

#### 预期结论
- 奖励模型提供更丰富的梯度信号
- 能区分"接近绕过"和"完全无效"的payload
- 在训练早期优势更明显

---

### RQ5: KL散度系数和奖励模型数据量如何影响效果？

#### 实验目标
研究两个关键超参数对ER和NRR的影响。

#### 超参数设置

| 参数 | 测试值 |
|------|--------|
| KL系数 β | 0, 0.1, 0.2, 0.5, 1.0 |
| 奖励模型数据量 | 2000, 4000 |

#### 实验配置

```python
RQ5_CONFIG = {
    "kl_coefficients": [0, 0.1, 0.2, 0.5, 1.0],
    "reward_data_sizes": [2000, 4000],
    "attack_types": ["sqli", "xss", "rce"],
    "metrics": ["er", "nrr", "ter"]
}
```

#### 评估指标矩阵

| β | 数据量 | ER | NRR | TER |
|---|--------|----|----|-----|
| 0 | 2000 | ? | ? | ? |
| 0 | 4000 | ? | ? | ? |
| 0.1 | 2000 | ? | ? | ? |
| ... | ... | ... | ... | ... |

#### 预期结论
- β=0: ER高但NRR低 (重复生成相同payload)
- β=1: NRR高但ER低 (太保守)
- β=0.2: 最佳平衡点
- 4000数据比2000数据效果更好

---

### RQ6: GPTFuzzer能否生成语法无法生成的新payload？

#### 实验目标
验证GPTFuzzer是否能超越原始语法，生成"新"的绕过payload。

#### 实验流程

```
1. 使用训练好的RL模型生成500K payload
2. 对每个payload判断是否能由原语法生成
3. 识别"新"payload (语法外)
4. 测试新payload的绕过率和功能性
```

#### 新颖性判定方法

```python
def is_novel(payload, grammar):
    """判断payload是否为语法无法生成的新payload"""
    try:
        # 尝试用语法解析
        parse_tree = grammar.parse(payload)
        return False  # 可以解析，不是新的
    except ParseError:
        return True   # 无法解析，是新的
```

#### 评估指标
- 新payload数量
- 新payload绕过率
- 新payload功能性验证率

#### 预期结论
- SQLi: 能生成新payload，约40%能绕过WAF且功能有效
- XSS/RCE: 因空间小，可能不生成新payload

---

## 五、实验执行计划

### 5.1 实验顺序

```
Phase 1: 基础模型训练
├── 1.1 SQLi预训练 (512K数据)
├── 1.2 XSS预训练 (512K数据)
└── 1.3 RCE预训练 (37K数据)

Phase 2: RQ3实验 (数据规模影响)
├── 2.1 准备不同规模数据集
├── 2.2 训练4组预训练模型
└── 2.3 评估并对比

Phase 3: 奖励模型训练
├── 3.1 生成WAF标注数据
├── 3.2 SQLi奖励模型 (4000样本)
├── 3.3 XSS奖励模型 (2000样本)
└── 3.4 RCE奖励模型 (2000样本)

Phase 4: RQ5实验 (超参数影响)
├── 4.1 测试不同KL系数
├── 4.2 测试不同数据量
└── 4.3 记录ER/NRR指标

Phase 5: RL训练
├── 5.1 SQLi RL (β=0.2)
├── 5.2 XSS RL (β=0.2)
└── 5.3 RCE RL (β=0.2)

Phase 6: RQ4实验 (奖励模型vs WAF)
├── 6.1 WAF直接反馈RL训练
└── 6.2 对比两种方式

Phase 7: RQ1实验 (有效性对比)
├── 7.1 实现基线方法
├── 7.2 各方法评估
└── 7.3 生成对比图表

Phase 8: RQ2实验 (语法影响)
├── 8.1 准备长/短序列数据
├── 8.2 训练对比模型
└── 8.3 评估效率差异

Phase 9: RQ6实验 (新颖性)
├── 9.1 生成500K payload
├── 9.2 新颖性判定
└── 9.3 功能性验证
```

### 5.2 预估时间

| 阶段 | RTX 4070 (8GB) | RTX 4090 (24GB) |
|------|----------------|-----------------|
| 预训练 (每种攻击) | 2-4小时 | 30-60分钟 |
| 奖励模型训练 | 15-30分钟 | 5-10分钟 |
| RL训练 | 2-4小时 | 30-60分钟 |
| WAF评估 | 取决于请求数 | 取决于请求数 |

---

## 六、结果输出格式

### 6.1 实验结果目录结构

```
results/
├── rq1_effectiveness/
│   ├── sqli_modsecurity.json
│   ├── sqli_naxsi.json
│   ├── xss_modsecurity.json
│   ├── xss_naxsi.json
│   ├── rce_modsecurity.json
│   ├── rce_naxsi.json
│   └── figures/
│       ├── tp_curves_sqli.png
│       ├── tp_curves_xss.png
│       └── tp_curves_rce.png
├── rq2_grammar/
│   ├── short_sequence_results.json
│   ├── long_sequence_results.json
│   └── figures/
├── rq3_data_scale/
│   ├── scale_0k.json
│   ├── scale_20k.json
│   ├── scale_256k.json
│   ├── scale_512k.json
│   └── figures/
├── rq4_reward_vs_waf/
│   ├── reward_model_results.json
│   ├── waf_guided_results.json
│   └── figures/
├── rq5_hyperparams/
│   ├── kl_ablation.json
│   ├── data_size_ablation.json
│   └── figures/
└── rq6_novel/
    ├── novel_payloads.json
    ├── grammar_coverage.json
    └── figures/
```

### 6.2 JSON结果格式

```json
{
    "experiment": "rq1_effectiveness",
    "attack_type": "sqli",
    "waf_type": "modsecurity",
    "method": "gptfuzzer",
    "timestamp": "2024-01-15T10:30:00",
    "config": {
        "request_budget": 1250000,
        "pretrain_data_size": 512000,
        "reward_data_size": 4000,
        "kl_coefficient": 0.2
    },
    "results": {
        "checkpoints": [
            {"requests": 10000, "tp": 150, "er": 0.015, "nrr": 0.85},
            {"requests": 50000, "tp": 720, "er": 0.0144, "nrr": 0.82},
            ...
        ],
        "final": {
            "total_requests": 1250000,
            "tp": 12500,
            "er": 0.01,
            "nrr": 0.78,
            "ter": 0.0098,
            "functional_rate": 0.99
        }
    }
}
```

---

## 七、代码文件说明

### 7.1 实验框架

| 文件 | 功能 |
|------|------|
| `experiments/__init__.py` | 模块初始化 |
| `experiments/experiment_config.py` | 实验配置管理 |
| `experiments/experiment_runner.py` | 统一实验运行器 |
| `experiments/metrics.py` | 评估指标计算 |
| `experiments/analysis.py` | 结果分析与可视化 |

### 7.2 基线方法

| 文件 | 功能 |
|------|------|
| `experiments/baselines/random_fuzzer.py` | 随机生成基线 |
| `experiments/baselines/grammar_fuzzer.py` | 语法RL基线 |

### 7.3 RQ实验脚本

| 文件 | 对应RQ |
|------|--------|
| `experiments/rq1_effectiveness.py` | RQ1: 有效性对比 |
| `experiments/rq2_grammar.py` | RQ2: 语法影响 |
| `experiments/rq3_data_scale.py` | RQ3: 数据规模 |
| `experiments/rq4_reward_vs_waf.py` | RQ4: 奖励模型vs WAF |
| `experiments/rq5_hyperparams.py` | RQ5: 超参数 |
| `experiments/rq6_novel_payloads.py` | RQ6: 新颖性 |

### 7.4 运行脚本

| 文件 | 功能 |
|------|------|
| `run_all_experiments.ps1` | 一键运行所有实验 |
| `run_rq1.ps1` ~ `run_rq6.ps1` | 单独运行各RQ |

---

## 八、注意事项

### 8.1 实验可复现性

- 固定随机种子 (seed=42)
- 记录所有超参数
- 保存中间检查点
- 每个实验重复5-10次取平均

### 8.2 资源管理

- 监控GPU显存使用
- 大规模实验分批进行
- 定期保存结果防止丢失

### 8.3 WAF测试注意

- 确保WAF服务稳定运行
- 设置合理的请求间隔避免过载
- 记录WAF响应状态和错误

---

## 九、参考文献

1. Liang et al., "Generative Pre-Trained Transformer-Based Reinforcement Learning for Testing Web Application Firewalls", IEEE TDSC 2024
2. Radford et al., "Language Models are Unsupervised Multitask Learners", OpenAI 2019
3. Schulman et al., "Proximal Policy Optimization Algorithms", arXiv 2017
